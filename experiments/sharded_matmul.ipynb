{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18877895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "\n",
    "import jax\n",
    "from jax.experimental import pallas as pl\n",
    "from jax.experimental.pallas import tpu as pltpu\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import NamedSharding, PartitionSpec as P\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b284a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update('jax_num_cpu_devices', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a5da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "def project_root() -> Path:\n",
    "    return Path(subprocess.check_output(\n",
    "        ['git', 'rev-parse', '--show-toplevel']\n",
    "    ).decode().strip())\n",
    "\n",
    "TRACES_DIR = project_root() / \"traces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79eb75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, k, n = 2048, 2048, 1024\n",
    "\n",
    "k1, k2 = jax.random.split(jax.random.key(0), 2)\n",
    "inputs = jax.random.normal(k1, (m, k), dtype=jnp.bfloat16)\n",
    "weights = jax.random.normal(k2, (k, n), dtype=jnp.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b67d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/my/845wytg53ln_99j7k_dzpl840000gn/T/ipykernel_85051/438086728.py:2: DeprecationWarning: The default axis_types will change in JAX v0.9.0 to jax.sharding.AxisType.Explicit. To maintain the old behavior, pass `axis_types=(jax.sharding.AxisType.Auto,) * len(axis_names)`. To opt-into the new behavior, pass `axis_types=(jax.sharding.AxisType.Explicit,) * len(axis_names)\n",
      "  mesh = jax.make_mesh((2, 2), (\"x\", \"y\"))\n"
     ]
    }
   ],
   "source": [
    "num_devices = jax.device_count()\n",
    "mesh = jax.make_mesh((2, 2), (\"x\", \"y\"))\n",
    "inp_sharding = jax.NamedSharding(mesh, P('x', 'y'))\n",
    "w_sharding = jax.NamedSharding(mesh, P('x', None))\n",
    "o_sharding = jax.NamedSharding(mesh, P('x', None))\n",
    "\n",
    "inputs = jax.device_put(inputs, inp_sharding)\n",
    "weights = jax.device_put(weights, w_sharding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c93d6",
   "metadata": {},
   "source": [
    "inputs are size 2048, 2048 -> bf16:: 2 bytes * 2048 * 2048 = ~8MB\n",
    "weights are size 2048, 1024 -> bf16:: 2 bytes * 2048 * 1024 = ~4MB\n",
    "\n",
    "inputs sharded along x and y -> $Inp[I_{X}, J_{Y}]$\n",
    "\n",
    "weights sharded along x -> $W[J_{X}, K]$\n",
    "\n",
    "Each device has N elements per array:\n",
    "  - inputs\n",
    "    - (2048 / 2) * (2048 / 2) * 2bytes\n",
    "    - ~2MB\n",
    "  - weights\n",
    "    - (2048 / 2) * 1024 * 2bytes\n",
    "    - ~2MB\n",
    "\n",
    "The contracting dimension is sharded in both inputs and weights, along different axes.\n",
    "Need to handle that with collectives; AG/AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e11eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   CPU 0    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   CPU 1    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">   CPU 2    </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">   CPU 3    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m   \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mCPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m    \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m   \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mCPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ea6c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0,1   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 2,3   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0,1\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 2,3\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eae0f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_matmul(x: jax.Array, y: jax.Array) -> jax.Array:\n",
    "    return jnp.matmul(x, y)\n",
    "\n",
    "out = basic_matmul(inputs, weights)\n",
    "compiled = jax.jit(basic_matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5a9fdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0,1   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 2,3   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0,1\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 2,3\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37b47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compiled(inputs, weights)\n",
    "result.block_until_ready()\n",
    "\n",
    "with jax.profiler.trace(TRACES_DIR):\n",
    "    result = compiled(inputs, weights)\n",
    "    result.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa631a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.shard_map,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")\n",
    "def xla_matmul(input_shard: jax.Array, w_shard: jax.Array) -> jax.Array:\n",
    "    # First we want to all_gather the data\n",
    "    with jax.named_scope('all_gather(s)'):\n",
    "        input_full = jax.lax.all_gather(input_shard, 'y', axis=1, tiled=True)\n",
    "        w_full = jax.lax.all_gather(w_shard, 'x', axis=0, tiled=True) # gather w along x\n",
    "    # Then we want to compute on the data\n",
    "    with jax.named_scope('dot'):\n",
    "        local_out = input_full @ w_full\n",
    "    # Then we want to all reduce the data\n",
    "    # with jax.named_scope('all_reduce'):\n",
    "    #     out = jax.lax.psum(local_out, 'y')\n",
    "    return local_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "950f9651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://docs.jax.dev/en/latest/notebooks/shard_map.html\n",
    "from jax.tree_util import tree_map, tree_all\n",
    "\n",
    "def allclose(a, b):\n",
    "  return tree_all(tree_map(functools.partial(jnp.allclose, atol=1e-2, rtol=1e-2), a, b))\n",
    "\n",
    "allclose(xla_matmul(inputs, weights), jnp.dot(inputs, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0e27b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemm2_compiled = jax.jit(xla_matmul)\n",
    "result = gemm2_compiled(inputs, weights)\n",
    "result.block_until_ready()\n",
    "\n",
    "with jax.profiler.trace(TRACES_DIR):\n",
    "    result = gemm2_compiled(inputs, weights)\n",
    "    result.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8035049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.shard_map,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")\n",
    "def xla_matmul2(input_shard: jax.Array, weight_shard: jax.Array) -> jax.Array:\n",
    "    \"\"\"\n",
    "    This time, we want to make the computation a little more efficient than\n",
    "    stacking the two all gathers are the beginning of the kernel\n",
    "    \"\"\"\n",
    "    y_idx = jax.lax.axis_index('y')\n",
    "    # All gather the weights over x so that each device contains full copy\n",
    "    w_full = jax.lax.all_gather(weight_shard, 'x', axis=0, tiled=True)\n",
    "    # Using the y-ring axis to determined which col stripe of weights to compute locally\n",
    "    w_slice = jax.lax.dynamic_slice(w_full, (y_idx * 1024, 0), (1024, 1024))\n",
    "    local_out = input_shard @ w_slice\n",
    "    # All Reduce over the y-ring to accumulate partial results\n",
    "    out = jax.lax.psum(local_out, 'y')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0113ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = xla_matmul2(inputs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1629ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allclose(basic_matmul(inputs, weights), xla_matmul2(inputs, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d0812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.shard_map,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")\n",
    "def xla_matmul3(input_shard: jax.Array, weight_shard: jax.Array) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Use some higher precision numerics to accomodate accumulation order\n",
    "    \"\"\"\n",
    "    y_idx = jax.lax.axis_index('y')\n",
    "    w_full = jax.lax.all_gather(weight_shard, 'x', axis=0, tiled=True)\n",
    "    # This shouldn't hardcode the dim shapes\n",
    "    w_slice = jax.lax.dynamic_slice(w_full, (y_idx * 1024, 0), (1024, 1024))\n",
    "    # This is probably overkill, think it might also incur perf penalty\n",
    "    #   - Something in the docs about precision highest\n",
    "    local_out = jax.lax.dot_general(\n",
    "        input_shard, w_slice,\n",
    "        dimension_numbers=(((1,), (0,)), ((), ())),\n",
    "        precision=jax.lax.Precision.HIGHEST,\n",
    "        preferred_element_type=jnp.float32,\n",
    "    )\n",
    "    out = jax.lax.psum(local_out, 'y')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39c84ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.allclose(xla_matmul3(inputs, weights), basic_matmul(inputs, weights), rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578eb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's recall what we've learned so far --\n",
    "\n",
    "When needed to perform an all gather on the reduction axis of our weights\n",
    "to remove the sharding over X\n",
    "\n",
    "Then, we compute local MatMuls (slicing out the appropriate data) between\n",
    "the shard local inputs and the full weights\n",
    "- Recall, we have the full weights after AG, so we need to slice out the\n",
    "  appropriate chunks of W for our computation\n",
    "\n",
    "These MatMuls are accumulators -> They finally need to be all reduced\n",
    "over Y. The desired out sharding is ('x', None), so when we do an AR\n",
    "over the Y axis, we are sharing the partial results\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Let's sketch out the algorithm we care about --\n",
    "\n",
    "We have 2 arrays distributed over 4 devices\n",
    "  - 1/4 of inputs on each device\n",
    "  - 1/2 of weights of each device\n",
    "\n",
    "We want to efficiently compute this distributed matmul over the devices\n",
    "\n",
    "We know that the contracting dims are sharded differently\n",
    "\n",
    "So there will need to be some comms to unshard so that we have the\n",
    "whole array in the right place. HOWEVER, we may also be able to get\n",
    "away with ppermute to simple pass _results_ after compute is finished\n",
    "\n",
    "Here are the kernels we may want to try\n",
    "  - Simple matmul with lax collectives inserted in the right spots\n",
    "  - MatMul with handrolled collectives (still AG to start, then AR)\n",
    "  - ppermute\n",
    "    - Issue async DMA\n",
    "    - Run local compute; stash in accumulator\n",
    "    - \n",
    "    - How much latency can we hide here?\n",
    "      - If the DMAs are fast/slow?\n",
    "      - How do we _reason_ about these tradeoffs\n",
    "\n",
    "Start with 2x2 case; don't worry too much about abstracting things out\n",
    "  - Then extend to larger configurations + abstractions\n",
    "  - How do we think about the work we're doing?\n",
    "  - Where are the opportunities to show different edge cases?\n",
    "  - Where do our assumptions break down?\n",
    "  - emit_pipeline\n",
    "  - kernel schedule?\n",
    "  - What happens when we run this on Trillium?\n",
    "    - What changes?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edb35ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here's what happens:\n",
    "- We have 2 arrays in HBM\n",
    "- Need to all_gather weights over x, now each device has fully copy of x\n",
    "- Then do local compute\n",
    "- All Reduce the local compute to get the correct results\n",
    "\"\"\"\n",
    "\n",
    "#NOTE: THIS IS THE CASE WHERE WE SIMPLY REPLACE jax.dot/jax.matmul/x @ y in xla_matmul3\n",
    "def simple_matmul(x_ref, y_ref, o_ref, scratch_ref, *, n_steps):\n",
    "  # Zero scratch buffer\n",
    "  @pl.when(pl.program_id(2) == 0)\n",
    "  def _init_scratch():\n",
    "    scratch_ref[...] = jnp.zeros_like(scratch_ref)\n",
    "\n",
    "  # Compute dot\n",
    "  scratch_ref[...] += jnp.dot(\n",
    "    x_ref[...],\n",
    "    y_ref[...],\n",
    "    preferred_element_type=jnp.float32\n",
    "  )\n",
    "\n",
    "  # Flush to HBM\n",
    "  @pl.when(pl.program_id(2) == n_steps - 1)\n",
    "  def _flush_scratch():\n",
    "    o_ref[...] = scratch_ref[...].astype(o_ref.dtype)\n",
    "\n",
    "\n",
    "def make_matmul(\n",
    "  x: jax.Array,\n",
    "  y: jax.Array,\n",
    "  *,\n",
    "  bm: int = 128,\n",
    "  bk: int = 128,\n",
    "  bn: int = 128,\n",
    "):\n",
    "  m, k = x.shape\n",
    "  _, n = y.shape\n",
    "\n",
    "  grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "    num_scalar_prefetch=0,\n",
    "    grid=(m//bm, n//bn, k//bk),\n",
    "    in_specs=[\n",
    "      pl.BlockSpec((bm, bk), lambda i,j,k: (i, k)),\n",
    "      pl.BlockSpec((bk, bn), lambda i,j,k: (k, j))\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),\n",
    "    scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)]\n",
    "  )\n",
    "\n",
    "  return pl.pallas_call(\n",
    "    functools.partial(simple_matmul, n_steps=k//bk),\n",
    "    grid_spec=grid_spec,\n",
    "    # Made this float32 to appease the numerics gods\n",
    "    out_shape=jax.ShapeDtypeStruct((m, n), dtype=jnp.float32),\n",
    "    interpret=True\n",
    "  )(x, y)\n",
    "\n",
    "\n",
    "def distributed_gemm_kernel1(inputs, weights):\n",
    "  y_idx = jax.lax.axis_index('y')\n",
    "  # AG\n",
    "  w_full = jax.lax.all_gather(weights, 'x', axis=0, tiled=True)\n",
    "  # jax.debug.print('w_full: {}', w_full.shape)\n",
    "  # Slice out local arrays\n",
    "  # TODO: again, fix these so they're not tied to the specific shapes\n",
    "  w_slice = jax.lax.dynamic_slice(w_full, (y_idx * 1024, 0), (1024, 1024))\n",
    "  # jax.debug.print('w_slice: {}', w_slice.shape)\n",
    "  # jax.debug.print('input_shape: {}', inputs.shape)\n",
    "  # We'll take the default tile sizes for now\n",
    "  local_out = make_matmul(inputs, w_slice)\n",
    "  # jax.debug.print('local_out: {}', local_out.shape)\n",
    "  return jax.lax.psum(local_out, 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96f9e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgk1 = jax.jit(\n",
    "    jax.shard_map(\n",
    "    distributed_gemm_kernel1,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f26d7cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.allclose(dgk1(inputs, weights), basic_matmul(inputs, weights), rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc718331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: 0.5\n",
      "mean diff: 0.05057927221059799\n",
      "median diff: 0.031246185302734375\n",
      "% > 0.1: 15.81%\n",
      "rows with errors: 2048  2048\n",
      "cols with errors: 1024 1024\n",
      "worst error at [875, 791]: ref=128, test=128.5\n",
      "top-left: mean=0.0507, % bad=15.9%\n",
      "top-right: mean=0.0506, % bad=15.8%\n",
      "bottom-left: mean=0.0505, % bad=15.8%\n",
      "bottom-right: mean=0.0505, % bad=15.7%\n"
     ]
    }
   ],
   "source": [
    "ref = basic_matmul(inputs, weights)\n",
    "test = dgk1(inputs, weights)\n",
    "diff = jnp.abs(ref - test)\n",
    "\n",
    "print(f\"max diff: {jnp.max(diff)}\")\n",
    "print(f\"mean diff: {jnp.mean(diff)}\")\n",
    "print(f\"median diff: {jnp.median(diff)}\")\n",
    "print(f\"% > 0.1: {100 * jnp.mean(diff > 0.1):.2f}%\")\n",
    "\n",
    "# Location of errors\n",
    "bad_mask = diff > 0.1\n",
    "bad_rows = jnp.any(bad_mask, axis=1)\n",
    "bad_cols = jnp.any(bad_mask, axis=0)\n",
    "print(f\"rows with errors: {jnp.sum(bad_rows)}  {ref.shape[0]}\")\n",
    "print(f\"cols with errors: {jnp.sum(bad_cols)} {ref.shape[1]}\")\n",
    "\n",
    "# Worst error location\n",
    "bad_idx = jnp.argmax(diff)\n",
    "i, j = bad_idx // ref.shape[1], bad_idx % ref.shape[1]\n",
    "print(f\"worst error at [{i}, {j}]: ref={ref[i,j]}, test={test[i,j]}\")\n",
    "\n",
    "# Check quadrants (shard boundaries)\n",
    "quadrants = [\n",
    "    (\"top-left\", diff[:1024, :512]),\n",
    "    (\"top-right\", diff[:1024, 512:]),\n",
    "    (\"bottom-left\", diff[1024:, :512]),\n",
    "    (\"bottom-right\", diff[1024:, 512:]),\n",
    "]\n",
    "for name, q in quadrants:\n",
    "    mean_val = float(jnp.mean(q))\n",
    "    pct_bad = float(100 * jnp.mean(q > 0.1))\n",
    "    print(f\"{name}: mean={mean_val:.4f}, % bad={pct_bad:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "448d42ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Running benchmark... done\n",
      "\n",
      "Results:\n",
      "  Mean:      199.003 ms\n",
      "  Median:      1.530 ms\n",
      "  Stdev:     272.175 ms\n",
      "  Min:         0.022 ms\n",
      "  P95:       650.769 ms\n",
      "  P99:       722.780 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import statistics\n",
    "\n",
    "for _ in range(3):\n",
    "    dgk1(inputs, weights).block_until_ready()\n",
    "print(\"done\")\n",
    "\n",
    "# Benchmark\n",
    "print(\"Running benchmark...\", end=\" \", flush=True)\n",
    "times = []\n",
    "for _ in range(50):\n",
    "    start = time.perf_counter()\n",
    "    dgk1(inputs, weights)\n",
    "    end = time.perf_counter()\n",
    "    times.append((end - start) * 1000)  # Convert to ms\n",
    "print(\"done\")\n",
    "print()\n",
    "\n",
    "# Statistics\n",
    "mean_ms = statistics.mean(times)\n",
    "median_ms = statistics.median(times)\n",
    "stdev_ms = statistics.stdev(times) if len(times) > 1 else 0\n",
    "min_ms = min(times)\n",
    "max_ms = max(times)\n",
    "p95 = np.percentile(times, 95)\n",
    "p99 = np.percentile(times, 99)\n",
    "\n",
    "print(\"Results:\")\n",
    "print(f\"  Mean:   {mean_ms:>10.3f} ms\")\n",
    "print(f\"  Median: {median_ms:>10.3f} ms\")\n",
    "print(f\"  Stdev:  {stdev_ms:>10.3f} ms\")\n",
    "print(f\"  Min:    {min_ms:>10.3f} ms\")\n",
    "print(f\"  P95:    {p95:>10.3f} ms\")\n",
    "print(f\"  P99:    {p99:>10.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think this one should have the matmul done after the full AG\n",
    "# Then we can interleave with ppermute\n",
    "\n",
    "# FIRST: Work out the AllGather\n",
    "# SECOND: Work out the MatMul\n",
    "# FINAL: Work out the AR\n",
    "\n",
    "\"\"\"\n",
    "On each local device, we have 2 arrays sitting there -> inputs + weights\n",
    "On kernel start:\n",
    "  - Barrier sync to get everyone on the same stage\n",
    "    - Might could relax this constraint?\n",
    "  - Issue remote DMAs along the ... ring to all gather the weights\n",
    "  - Once received\n",
    "    - Compute local dots\n",
    "    - Accumulate\n",
    "  - Finally\n",
    "    - All Reduce over the y ring to conform shmap shape\n",
    "\"\"\"\n",
    "\n",
    "def all_gather_kernel_1D(\n",
    "  input_ref, output_ref,\n",
    "  local_send_sem, send_sem, recv_sem, \n",
    "  # ...\n",
    "):\n",
    "  \"\"\"\n",
    "  These two refs are intended to decouple what's going on (that's the best you can do?)\n",
    "\n",
    "  The input ref is the local ref, the output ref is the ref that will be\n",
    "  sending/receiving data from our neighboring devices\n",
    "  \"\"\"\n",
    "  #TODO: Barrier\n",
    "  \n",
    "  # Get tensor dims/sizes\n",
    "  # This should be baked into compiled artifact? Or is it runtime?\n",
    "  # There has to be a prettier way to do this?\n",
    "  shard_height = input_ref.shape[0]\n",
    "  shard_width = input_ref.shape[1]\n",
    "\n",
    "  # Get neighbors\n",
    "  # Map to some position in [(0,0), (1,0), (0,1), (1,1)] along x\n",
    "  # left_dev = jax.lax.rem(device_id - 1, x_ring)\n",
    "  this_device_x = jax.lax.axis_index('x')\n",
    "  this_device_y = jax.lax.axis_index('y')\n",
    "  x_ring = jax.lax.axis_size('x')\n",
    "  y_ring = jax.lax.axis_size('y')\n",
    "  right_dev = jax.lax.rem(this_device_x + 1, x_ring)\n",
    "\n",
    "  # Hard coding 2: where 2 is supposed to be the ring_length\n",
    "  neighbor_x = (this_device_x + 1) % x_ring\n",
    "  # neighbor_linear = neighbor_x * y_ring + this_device_y\n",
    "\n",
    "  # PERFORM INITIAL ASYNC COPY FROM OUR HBM TO OUR HBM\n",
    "  # @pl.when(pl.program_id(0) == 0) -> We're just copying within our HBM to a bigger HBM memory\n",
    "  # XLA liveness should handle malloc/free the _INPUT_ tensor once the AG completes\n",
    "  #   def _copy_local_to_local \n",
    "  local_hbm_copy = pltpu.make_async_copy(\n",
    "    src_ref=input_ref,\n",
    "    dst_ref=output_ref.at[pl.ds(this_device_x * shard_height, shard_height), :],\n",
    "    sem=local_send_sem\n",
    "  )\n",
    "\n",
    "  # We can defer the wait until literally the very end of the kernel\n",
    "  local_hbm_copy.start()\n",
    "\n",
    "  # Issue RDMA\n",
    "  #NOTE: This is buggy; this won't work for axis lengths > 2\n",
    "  # Logic would keep writing to the same location in the neighbor's out_ref*\n",
    "  right_dma = pltpu.make_async_remote_copy(\n",
    "    src_ref=input_ref.at[...],\n",
    "    dst_ref=output_ref.at[pl.ds(this_device_x * shard_height, shard_height), :],\n",
    "    send_sem=send_sem,\n",
    "    recv_sem=recv_sem,\n",
    "    #NOTE: device_id has to match the mesh specs\n",
    "    # Since we're in a 2x2 grid -> Need to communication _which_ links we're using\n",
    "    # device_id=(right_dev,),\n",
    "    device_id_type=pltpu.DeviceIdType.MESH,\n",
    "    device_id=(right_dev, this_device_y),\n",
    "    # device_id=(right_dev),\n",
    "    # device_id=neighbor_linear,\n",
    "    # device_id_type=pltpu.DeviceIdType.LOGICAL,\n",
    "  )\n",
    "\n",
    "  # Wait on RDMA (send/recv)\n",
    "  right_dma.start()\n",
    "  right_dma.wait()\n",
    "  local_hbm_copy.wait()\n",
    "\n",
    "grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "  num_scalar_prefetch=0,\n",
    "  # This logic is wrong -> this is not a \"line\" of devices, but a 2x2 grid\n",
    "  # We only want to iterate along the number of devices PER RING - 1 times\n",
    "  grid=(1,), # Could we move this elsewhere?\n",
    "  in_specs=[\n",
    "    # Our input reference is just our big tensor in HBM\n",
    "    pl.BlockSpec(memory_space=pl.ANY)\n",
    "  ],\n",
    "  # Our output reference will be _another_ big tensor in HBM\n",
    "  out_specs=pl.BlockSpec(memory_space=pl.ANY),\n",
    "  # This will be an error if you need more semaphores for more neighbors\n",
    "  scratch_shapes=(\n",
    "    [pltpu.SemaphoreType.DMA] * 3\n",
    "  )\n",
    ")\n",
    "\n",
    "out_shape=jax.ShapeDtypeStruct((weights.shape), dtype=jnp.bfloat16)\n",
    "\n",
    "def make_ag(x):\n",
    "  # TODO: should we parameterize this _here_; aka pass in the shard shape data here?\n",
    "  return pl.pallas_call(\n",
    "    all_gather_kernel_1D,\n",
    "    grid_spec=grid_spec,\n",
    "    out_shape=out_shape,\n",
    "    interpret=True\n",
    "  )(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3954349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basically --\n",
    "The way we organize the grid for communications is a decision\n",
    "  - Rings, ranges, etc.\n",
    "  - This trades off bandwidth/latency\n",
    "\n",
    "\n",
    "# Ring\n",
    "grid = (ring_size - 1,)\n",
    "\n",
    "# Recursive doubling\n",
    "grid = (log2(ring_size),)\n",
    "\n",
    "# Direct\n",
    "grid = (1,)  # but more complex RDMA pattern\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5bb445c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xla_allgather(weights):\n",
    "    w_full = jax.lax.all_gather(weights, 'x', axis=0, tiled=True)\n",
    "    return w_full\n",
    "\n",
    "res = jax.jit(\n",
    "    jax.shard_map(\n",
    "        xla_allgather,\n",
    "        mesh=mesh,\n",
    "        in_specs=P('x', None),\n",
    "        out_specs=P(),\n",
    "        check_vma=False\n",
    "    )\n",
    ")(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e710440e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Meshes with more than 1 named dimension not implemented in dma_start_p",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJaxStackTraceBeforeTransformation\u001b[39m         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen runpy>:198\u001b[39m, in \u001b[36m_run_module_as_main\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen runpy>:88\u001b[39m, in \u001b[36m_run_code\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/ipykernel_launcher.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipykernel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m kernelapp \u001b[38;5;28;01mas\u001b[39;00m app\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m app.launch_new_instance()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/traitlets/config/application.py:1075\u001b[39m, in \u001b[36mlaunch_instance\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1074\u001b[39m app.initialize(argv)\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m app.start()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/ipykernel/kernelapp.py:758\u001b[39m, in \u001b[36mstart\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28mself\u001b[39m.io_loop.start()\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/tornado/platform/asyncio.py:211\u001b[39m, in \u001b[36mstart\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28mself\u001b[39m.asyncio_loop.run_forever()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.5-macos-aarch64-none/lib/python3.13/asyncio/base_events.py:683\u001b[39m, in \u001b[36mrun_forever\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    682\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_once()\n\u001b[32m    684\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.5-macos-aarch64-none/lib/python3.13/asyncio/base_events.py:2042\u001b[39m, in \u001b[36m_run_once\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   2041\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2042\u001b[39m         handle._run()\n\u001b[32m   2043\u001b[39m handle = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.5-macos-aarch64-none/lib/python3.13/asyncio/events.py:89\u001b[39m, in \u001b[36m_run\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28mself\u001b[39m._context.run(\u001b[38;5;28mself\u001b[39m._callback, *\u001b[38;5;28mself\u001b[39m._args)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/ipykernel/kernelbase.py:614\u001b[39m, in \u001b[36mshell_main\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m asyncio_lock:\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/ipykernel/kernelbase.py:471\u001b[39m, in \u001b[36mdispatch_shell\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m result\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/ipykernel/ipkernel.py:366\u001b[39m, in \u001b[36mexecute_request\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Override for cell output - cell reconciliation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().execute_request(stream, ident, parent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/ipykernel/kernelbase.py:827\u001b[39m, in \u001b[36mexecute_request\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(reply_content):\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m     reply_content = \u001b[38;5;28;01mawait\u001b[39;00m reply_content\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/ipykernel/ipkernel.py:458\u001b[39m, in \u001b[36mdo_execute\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accepts_params[\u001b[33m\"\u001b[39m\u001b[33mcell_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     res = shell.run_cell(\n\u001b[32m    459\u001b[39m         code,\n\u001b[32m    460\u001b[39m         store_history=store_history,\n\u001b[32m    461\u001b[39m         silent=silent,\n\u001b[32m    462\u001b[39m         cell_id=cell_id,\n\u001b[32m    463\u001b[39m     )\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/ipykernel/zmqshell.py:663\u001b[39m, in \u001b[36mrun_cell\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    662\u001b[39m \u001b[38;5;28mself\u001b[39m._last_traceback = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().run_cell(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3123\u001b[39m, in \u001b[36mrun_cell\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3122\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3123\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._run_cell(\n\u001b[32m   3124\u001b[39m         raw_cell, store_history, silent, shell_futures, cell_id\n\u001b[32m   3125\u001b[39m     )\n\u001b[32m   3126\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3178\u001b[39m, in \u001b[36m_run_cell\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3177\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3178\u001b[39m     result = runner(coro)\n\u001b[32m   3179\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/IPython/core/async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3400\u001b[39m, in \u001b[36mrun_cell_async\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3397\u001b[39m interactivity = \u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m silent \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ast_node_interactivity\n\u001b[32m-> \u001b[39m\u001b[32m3400\u001b[39m has_raised = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_ast_nodes(code_ast.body, cell_name,\n\u001b[32m   3401\u001b[39m        interactivity=interactivity, compiler=compiler, result=result)\n\u001b[32m   3403\u001b[39m \u001b[38;5;28mself\u001b[39m.last_execution_succeeded = \u001b[38;5;129;01mnot\u001b[39;00m has_raised\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3641\u001b[39m, in \u001b[36mrun_ast_nodes\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3640\u001b[39m     asy = compare(code)\n\u001b[32m-> \u001b[39m\u001b[32m3641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_code(code, result, async_=asy):\n\u001b[32m   3642\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3701\u001b[39m, in \u001b[36mrun_code\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3700\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3701\u001b[39m         exec(code_obj, \u001b[38;5;28mself\u001b[39m.user_global_ns, \u001b[38;5;28mself\u001b[39m.user_ns)\n\u001b[32m   3702\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   3703\u001b[39m     \u001b[38;5;66;03m# Reset our crash handler in place\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[107]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res2 = jax.jit(\n\u001b[32m      2\u001b[39m     jax.shard_map(\n\u001b[32m      3\u001b[39m         make_ag,\n\u001b[32m      4\u001b[39m         mesh=mesh,\n\u001b[32m      5\u001b[39m         in_specs=P(\u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m      6\u001b[39m         out_specs=P(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m      7\u001b[39m         check_vma=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      8\u001b[39m     )\n\u001b[32m      9\u001b[39m )(weights)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mmake_ag\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_ag\u001b[39m(x):\n\u001b[32m    107\u001b[39m   \u001b[38;5;66;03m# TODO: should we parameterize this _here_; aka pass in the shard shape data here?\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m pl.pallas_call(\n\u001b[32m    109\u001b[39m     all_gather_kernel_1D,\n\u001b[32m    110\u001b[39m     grid_spec=grid_spec,\n\u001b[32m    111\u001b[39m     out_shape=out_shape,\n\u001b[32m    112\u001b[39m     interpret=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    113\u001b[39m   )(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/jax/_src/pallas/pallas_call.py:1533\u001b[39m, in \u001b[36mwrapped\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1532\u001b[39m   kernel_dbg = kernel_dbg.replace_func_name(mlir.sanitize_name(name))\n\u001b[32m-> \u001b[39m\u001b[32m1533\u001b[39m jaxpr, consts = _trace_kernel_to_jaxpr(\n\u001b[32m   1534\u001b[39m     kernel, kernel_dbg, grid_mapping, flat_kernel_avals,\n\u001b[32m   1535\u001b[39m     kernel_in_tree, kernel_arg_transforms)\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i_idx, o_idx \u001b[38;5;129;01min\u001b[39;00m input_output_aliases.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/jax/_src/pallas/pallas_call.py:1007\u001b[39m, in \u001b[36m_trace_kernel_to_jaxpr\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config.mutable_array_checks(\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m   jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(\n\u001b[32m   1008\u001b[39m       wrapped_kernel_fun, kernel_avals)\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m consts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/jax/_src/pallas/primitives.py:851\u001b[39m, in \u001b[36mwrap_with_transforms\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    847\u001b[39m new_args = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    848\u001b[39m     state_types.TransformedRef(a, t) \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;28;01melse\u001b[39;00m a\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m a, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, transforms)\n\u001b[32m    850\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m851\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m f(*new_args)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mall_gather_kernel_1D\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Wait on RDMA (send/recv)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m right_dma.start()\n\u001b[32m     85\u001b[39m right_dma.wait()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/jax/_src/pallas/mosaic/primitives.py:242\u001b[39m, in \u001b[36mstart\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    241\u001b[39m flat_args, tree = \u001b[38;5;28mself\u001b[39m._get_args_and_tree()\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m dma_start_p.bind(\n\u001b[32m    243\u001b[39m     *flat_args,\n\u001b[32m    244\u001b[39m     tree=tree,\n\u001b[32m    245\u001b[39m     device_id_type=\u001b[38;5;28mself\u001b[39m.device_id_type,\n\u001b[32m    246\u001b[39m     priority=priority,\n\u001b[32m    247\u001b[39m     add=add,\n\u001b[32m    248\u001b[39m )\n",
      "\u001b[31mJaxStackTraceBeforeTransformation\u001b[39m: NotImplementedError: Meshes with more than 1 named dimension not implemented in dma_start_p\n\nThe preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\n\n--------------------",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[107]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res2 = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshard_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmake_ag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43min_specs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mP\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout_specs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mP\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_vma\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 23 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/jax/_src/pallas/pallas_call.py:1071\u001b[39m, in \u001b[36m_pallas_call_lowering\u001b[39m\u001b[34m(ctx, interpret, backend, *in_nodes, **params)\u001b[39m\n\u001b[32m   1067\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1068\u001b[39m     impl = partial(hlo_interpreter.pallas_call_hlo_interpret,\n\u001b[32m   1069\u001b[39m                    backend=backend,\n\u001b[32m   1070\u001b[39m                    **params)\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmlir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiple_results\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43min_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcpu_lowering\u001b[39m(ctx: mlir.LoweringRuleContext,\n\u001b[32m   1074\u001b[39m                  *in_nodes: mlir.ir.Value | Sequence[mlir.ir.Value],\n\u001b[32m   1075\u001b[39m                  **params):\n\u001b[32m   1076\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mOnly interpret mode is supported on CPU backend.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 4 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/jax/_src/pallas/hlo_interpreter.py:375\u001b[39m, in \u001b[36mpallas_call_hlo_interpret\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    369\u001b[39m grid = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    370\u001b[39m     a \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pallas_core.dynamic_grid_dim\n\u001b[32m    371\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(dynamic_grid_args_iter)\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m grid_mapping.grid\n\u001b[32m    373\u001b[39m )\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(dynamic_grid_args_iter, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m discharged_jaxpr, discharged_consts, scratch_avals = \u001b[43mkernel_to_hlo_jaxpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[32m    378\u001b[39m   \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mJaxpr of the the kernel in pallas_call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdebug_info.func_src_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/jax/_src/pallas/hlo_interpreter.py:176\u001b[39m, in \u001b[36mkernel_to_hlo_jaxpr\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    174\u001b[39m   scratch_invars = phys_jaxpr.invars[grid_mapping.slice_scratch_ops]\n\u001b[32m    175\u001b[39m   scratch_avals = [v.aval \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m scratch_invars]\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m   discharged_jaxpr, discharged_consts = \u001b[43mstate_discharge\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdischarge_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m      \u001b[49m\u001b[43mphys_jaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphys_consts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m discharged_jaxpr, discharged_consts, scratch_avals\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/jax/_src/state/discharge.py:90\u001b[39m, in \u001b[36mdischarge_state\u001b[39m\u001b[34m(jaxpr, consts, should_discharge)\u001b[39m\n\u001b[32m     84\u001b[39m in_avals = [v.aval.inner_aval\n\u001b[32m     85\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v.aval, AbstractRef) \u001b[38;5;129;01mand\u001b[39;00m d\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m v.aval \u001b[38;5;28;01mfor\u001b[39;00m v, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(jaxpr.invars, should_discharge)]\n\u001b[32m     87\u001b[39m eval_jaxpr = lu.wrap_init(partial(_eval_jaxpr_discharge_state, jaxpr,\n\u001b[32m     88\u001b[39m                                   should_discharge, consts),\n\u001b[32m     89\u001b[39m                           debug_info=jaxpr.debug_info.with_unknown_names())\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m new_jaxpr, _ , new_consts = \u001b[43mpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_to_jaxpr_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_jaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_avals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_jaxpr, new_consts\n",
      "    \u001b[31m[... skipping hidden 3 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/jax/_src/state/discharge.py:233\u001b[39m, in \u001b[36m_eval_jaxpr_discharge_state\u001b[39m\u001b[34m(jaxpr, should_discharge, consts, *args)\u001b[39m\n\u001b[32m    231\u001b[39m in_avals = [v.aval \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m eqn.invars]\n\u001b[32m    232\u001b[39m out_avals = [v.aval \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m eqn.outvars]\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m new_invals, ans = \u001b[43mrule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_avals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_avals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m invar, should, new_inval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eqn.invars, should_discharge, new_invals):\n\u001b[32m    236\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m new_inval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/jax/_src/pallas/mosaic/primitives.py:619\u001b[39m, in \u001b[36mdma_start_partial_discharge_rule\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    615\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    616\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdevice_id (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_id_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) and mesh (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(axis_env.axis_sizes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    617\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mmust have same length.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_id_len > \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nonempty_axes) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMeshes with more than 1 named dimension not \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    620\u001b[39m                             \u001b[33m\"\u001b[39m\u001b[33mimplemented in dma_start_p\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    621\u001b[39m shard_axis = nonempty_axes[\u001b[32m0\u001b[39m]\n\u001b[32m    622\u001b[39m my_axis = jax.lax.axis_index(shard_axis)\n",
      "\u001b[31mNotImplementedError\u001b[39m: Meshes with more than 1 named dimension not implemented in dma_start_p"
     ]
    }
   ],
   "source": [
    "res2 = jax.jit(\n",
    "    jax.shard_map(\n",
    "        make_ag,\n",
    "        mesh=mesh,\n",
    "        in_specs=P('x', None),\n",
    "        out_specs=P(None, None),\n",
    "        check_vma=False\n",
    "    )\n",
    ")(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc015c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here's something fun --\n",
    "\n",
    "NotImplementedError: Meshes with more than 1 named\n",
    "dimension not implemented in dma_start_p\n",
    "\n",
    "Pallas's remote DMA primitives currently only support 1D meshes. That's a real limitation.\n",
    "\n",
    "https://github.com/jax-ml/jax/blob/main/jax/_src/pallas/mosaic/primitives.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_shmap_ag_kernel(\n",
    "    input_ref, output_ref,\n",
    "    local_copy_sem, send_sem, recv_sem\n",
    "):\n",
    "  # Get axis detail\n",
    "  # Make Local HBM <--> HBM copy\n",
    "  # Make Remote HBM <--> HBM copy\n",
    "\n",
    "  pass\n",
    "\n",
    "# def no_shmap_ag(input):\n",
    "#     return pl.pallas_call(\n",
    "#         kernel,\n",
    "#         grid_spec=(),\n",
    "#         out_shape=(),\n",
    "        \n",
    "#     )(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_gather_kernel_2D(\n",
    "  x\n",
    "):\n",
    "  # Barrier\n",
    "  \n",
    "  # Get neighbors\n",
    "\n",
    "  # Issue RDMA\n",
    "\n",
    "  # Wait on RDMA (send/recv)\n",
    "  # NOTE: need to change the semaphores/refs to handle reads/writes from multiple neighbors\n",
    "  # NOTE: In a 2x2 grid, there's only one neighbor**\n",
    "  # Does this need to be accounted for in code? -> For loop type construct to handle this  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09deb5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_all_gather():\n",
    "    pass\n",
    "\n",
    "\n",
    "res = jax.jit(\n",
    "    jax.shard_map(\n",
    "        new_all_gather,\n",
    "        mesh=mesh,\n",
    "        in_specs=(P(), P()),\n",
    "        out_specs=P(),\n",
    "        # check_vma=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.shard_map(\n",
    "#     ...\n",
    "# )(inputs, weights)\n",
    "\n",
    "# emit_pipeline version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3365c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUT IT TOGETHER\n",
    "# INTERLEAVE THE COMPUTE with ppermute\n",
    "# emit_pipeline??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb556444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMERICS CHECK\n",
    "\n",
    "# ACCURACY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c628a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROFILING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTEND THE KERNEL TO BE MORE GENERAL THAN 2x2\n",
    "# REPEAT CHECKS\n",
    "# SCALE UP TO 4x4 GRID"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
