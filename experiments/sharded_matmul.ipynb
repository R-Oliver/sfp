{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18877895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "\n",
    "import jax\n",
    "from jax.experimental import pallas as pl\n",
    "from jax.experimental.pallas import tpu as pltpu\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import NamedSharding, PartitionSpec as P\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sfp.utils import benchmark, numerics, profile, upload_to_gcs\n",
    "\n",
    "jax.config.update('jax_num_cpu_devices', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79eb75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, k, n = 2048, 2048, 1024\n",
    "\n",
    "k1, k2 = jax.random.split(jax.random.key(0), 2)\n",
    "inputs = jax.random.normal(k1, (m, k), dtype=jnp.bfloat16)\n",
    "weights = jax.random.normal(k2, (k, n), dtype=jnp.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b67d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/my/845wytg53ln_99j7k_dzpl840000gn/T/ipykernel_11568/438086728.py:2: DeprecationWarning: The default axis_types will change in JAX v0.9.0 to jax.sharding.AxisType.Explicit. To maintain the old behavior, pass `axis_types=(jax.sharding.AxisType.Auto,) * len(axis_names)`. To opt-into the new behavior, pass `axis_types=(jax.sharding.AxisType.Explicit,) * len(axis_names)\n",
      "  mesh = jax.make_mesh((2, 2), (\"x\", \"y\"))\n"
     ]
    }
   ],
   "source": [
    "num_devices = jax.device_count()\n",
    "mesh = jax.make_mesh((2, 2), (\"x\", \"y\"))\n",
    "inp_sharding = jax.NamedSharding(mesh, P('x', 'y'))\n",
    "w_sharding = jax.NamedSharding(mesh, P('x', None))\n",
    "o_sharding = jax.NamedSharding(mesh, P('x', None))\n",
    "\n",
    "inputs = jax.device_put(inputs, inp_sharding)\n",
    "weights = jax.device_put(weights, w_sharding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c93d6",
   "metadata": {},
   "source": [
    "inputs are size 2048, 2048 -> bf16:: 2 bytes * 2048 * 2048 = ~8MB\n",
    "weights are size 2048, 1024 -> bf16:: 2 bytes * 2048 * 1024 = ~4MB\n",
    "\n",
    "inputs sharded along x and y -> $Inp[I_{X}, J_{Y}]$\n",
    "\n",
    "weights sharded along x -> $W[J_{X}, K]$\n",
    "\n",
    "Each device has N elements per array:\n",
    "  - inputs\n",
    "    - (2048 / 2) * (2048 / 2) * 2bytes\n",
    "    - ~2MB\n",
    "  - weights\n",
    "    - (2048 / 2) * 1024 * 2bytes\n",
    "    - ~2MB\n",
    "\n",
    "The contracting dimension is sharded in both inputs and weights, along different axes.\n",
    "Need to handle that with collectives; AG/AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81e11eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   CPU 0    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   CPU 1    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">   CPU 2    </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">   CPU 3    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m   \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mCPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m    \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m   \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mCPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30ea6c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0,1   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 2,3   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0,1\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 2,3\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa631a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jax_matmul(x: jax.Array, y: jax.Array) -> jax.Array:\n",
    "    return jnp.matmul(x, y)\n",
    "\n",
    "@functools.partial(\n",
    "    jax.shard_map,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")\n",
    "def xla_matmul_1(input_shard: jax.Array, w_shard: jax.Array) -> jax.Array:\n",
    "    # First we want to all_gather the data\n",
    "    with jax.named_scope('all_gather(s)'):\n",
    "        input_full = jax.lax.all_gather(input_shard, 'y', axis=1, tiled=True)\n",
    "        w_full = jax.lax.all_gather(w_shard, 'x', axis=0, tiled=True) # gather w along x\n",
    "    # Then we want to compute on the data\n",
    "    # Since we did two full all gathers to start, regular GEMMs\n",
    "    with jax.named_scope('dot'):\n",
    "        local_out = input_full @ w_full\n",
    "    return local_out\n",
    "\n",
    "jnp.allclose(xla_matmul_1(inputs, weights), jnp.dot(inputs, weights), atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "950f9651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0,1   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 2,3   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0,1\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 2,3\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = jax_matmul(inputs, weights)\n",
    "jax_matmul_compiled = jax.jit(jax_matmul)\n",
    "jax.debug.visualize_array_sharding(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e27b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "jmc = jax_matmul_compiled(inputs, weights)\n",
    "jmc.block_until_ready()\n",
    "\n",
    "with jax.profiler.trace(TRACES_DIR):\n",
    "    result = jax_matmul_compiled(inputs, weights)\n",
    "    result.block_until_ready()\n",
    "\n",
    "xm1 = jax.jit(xla_matmul_1)\n",
    "result = xm1(inputs, weights)\n",
    "result.block_until_ready()\n",
    "\n",
    "with jax.profiler.trace(TRACES_DIR):\n",
    "    result = xm1(inputs, weights)\n",
    "    result.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8035049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.shard_map,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")\n",
    "def xla_matmul_2(input_shard: jax.Array, weight_shard: jax.Array) -> jax.Array:\n",
    "    \"\"\"\n",
    "    This time, we want to make the computation a little more efficient than\n",
    "    stacking the two all gathers at the beginning of the kernel\n",
    "\n",
    "    All Reduce at the end over partial sums\n",
    "    \"\"\"\n",
    "    y_idx = jax.lax.axis_index('y')\n",
    "    # All gather the weights over x so that each device contains full copy\n",
    "    w_full = jax.lax.all_gather(weight_shard, 'x', axis=0, tiled=True)\n",
    "    # Using the y-ring axis to determined which col stripe of weights to compute locally\n",
    "    w_slice = jax.lax.dynamic_slice(w_full, (y_idx * 1024, 0), (1024, 1024))\n",
    "    local_out = input_shard @ w_slice\n",
    "    # All Reduce over the y-ring to accumulate partial results\n",
    "    out = jax.lax.psum(local_out, 'y')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0113ffbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(False, dtype=bool)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.allclose(jax_matmul(inputs, weights), xla_matmul_2(inputs, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8d0812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.shard_map,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")\n",
    "def xla_matmul_3(input_shard: jax.Array, weight_shard: jax.Array) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Use some higher precision numerics to demonstrate accumulation order (fp32)\n",
    "    \"\"\"\n",
    "    y_idx = jax.lax.axis_index('y')\n",
    "    w_full = jax.lax.all_gather(weight_shard, 'x', axis=0, tiled=True)\n",
    "    # This shouldn't hardcode the dim shapes\n",
    "    w_slice = jax.lax.dynamic_slice(w_full, (y_idx * 1024, 0), (1024, 1024))\n",
    "    # This is probably overkill, think it might also incur perf penalty\n",
    "    #   - Something in the docs about precision highest\n",
    "    local_out = jax.lax.dot_general(\n",
    "        input_shard, w_slice,\n",
    "        dimension_numbers=(((1,), (0,)), ((), ())),\n",
    "        precision=jax.lax.Precision.HIGHEST,\n",
    "        preferred_element_type=jnp.float32,\n",
    "    )\n",
    "    out = jax.lax.psum(local_out, 'y')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39c84ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.allclose(xla_matmul_3(inputs, weights), jax_matmul(inputs, weights), rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: INCLUDE A VERSION WITH SOME XLA COMPILER FLAGS ENABLED TO TEST PERFORMANCE OR ASYNC ALL_GATHER, ETC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578eb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's recall what we've learned so far --\n",
    "\n",
    "When needed to perform an all gather on the reduction axis of our weights\n",
    "to remove the sharding over X\n",
    "\n",
    "Then, we compute local MatMuls (slicing out the appropriate data) between\n",
    "the shard local inputs and the full weights\n",
    "- Recall, we have the full weights after AG, so we need to slice out the\n",
    "  appropriate chunks of W for our computation\n",
    "\n",
    "These MatMuls are accumulators -> They finally need to be all reduced\n",
    "over Y. The desired out sharding is ('x', None), so when we do an AR\n",
    "over the Y axis, we are sharing the partial results\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Let's sketch out the algorithm we care about --\n",
    "\n",
    "We have 2 arrays distributed over 4 devices\n",
    "  - 1/4 of inputs on each device\n",
    "  - 1/2 of weights of each device\n",
    "\n",
    "We want to efficiently compute this distributed matmul over the devices\n",
    "\n",
    "We know that the contracting dims are sharded differently\n",
    "\n",
    "So there will need to be some comms to unshard so that we have the\n",
    "whole array in the right place. HOWEVER, we may also be able to get\n",
    "away with ppermute to simple pass _results_ after compute is finished\n",
    "\n",
    "Here are the kernels we may want to try\n",
    "  - Simple matmul with lax collectives inserted in the right spots\n",
    "  - MatMul with handrolled collectives (still AG to start, then AR)\n",
    "  - ppermute\n",
    "    - Issue async DMA\n",
    "    - Run local compute; stash in accumulator\n",
    "    - \n",
    "    - How much latency can we hide here?\n",
    "      - If the DMAs are fast/slow?\n",
    "      - How do we _reason_ about these tradeoffs\n",
    "\n",
    "Start with 2x2 case; don't worry too much about abstracting things out\n",
    "  - Then extend to larger configurations + abstractions\n",
    "  - How do we think about the work we're doing?\n",
    "  - Where are the opportunities to show different edge cases?\n",
    "  - Where do our assumptions break down?\n",
    "  - emit_pipeline\n",
    "  - kernel schedule?\n",
    "  - What happens when we run this on Trillium?\n",
    "    - What changes?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edb35ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here's what happens:\n",
    "- We have 2 arrays in HBM\n",
    "- Need to all_gather weights over x, now each device has fully copy of x\n",
    "- Then do local compute\n",
    "- All Reduce the local compute to get the correct results\n",
    "\"\"\"\n",
    "\n",
    "#NOTE: THIS IS THE CASE WHERE WE SIMPLY REPLACE jax.dot/jax.matmul/x @ y in xla_matmul3\n",
    "def simple_matmul(x_ref, y_ref, o_ref, scratch_ref, *, n_steps):\n",
    "  # Zero scratch buffer\n",
    "  @pl.when(pl.program_id(2) == 0)\n",
    "  def _init_scratch():\n",
    "    scratch_ref[...] = jnp.zeros_like(scratch_ref)\n",
    "\n",
    "  # Compute dot\n",
    "  scratch_ref[...] += jnp.dot(\n",
    "    x_ref[...],\n",
    "    y_ref[...],\n",
    "    preferred_element_type=jnp.float32\n",
    "  )\n",
    "\n",
    "  # Flush to HBM\n",
    "  @pl.when(pl.program_id(2) == n_steps - 1)\n",
    "  def _flush_scratch():\n",
    "    o_ref[...] = scratch_ref[...].astype(o_ref.dtype)\n",
    "\n",
    "\n",
    "def make_matmul(\n",
    "  x: jax.Array,\n",
    "  y: jax.Array,\n",
    "  *,\n",
    "  bm: int = 128,\n",
    "  bk: int = 128,\n",
    "  bn: int = 128,\n",
    "):\n",
    "  m, k = x.shape\n",
    "  _, n = y.shape\n",
    "\n",
    "  grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "    num_scalar_prefetch=0,\n",
    "    grid=(m//bm, n//bn, k//bk),\n",
    "    in_specs=[\n",
    "      pl.BlockSpec((bm, bk), lambda i,j,k: (i, k)),\n",
    "      pl.BlockSpec((bk, bn), lambda i,j,k: (k, j))\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),\n",
    "    scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)]\n",
    "  )\n",
    "\n",
    "  return pl.pallas_call(\n",
    "    functools.partial(simple_matmul, n_steps=k//bk),\n",
    "    grid_spec=grid_spec,\n",
    "    # Made this float32 to appease the numerics gods\n",
    "    out_shape=jax.ShapeDtypeStruct((m, n), dtype=jnp.float32),\n",
    "    interpret=True\n",
    "  )(x, y)\n",
    "\n",
    "\n",
    "def distributed_gemm_kernel1(inputs, weights):\n",
    "  y_idx = jax.lax.axis_index('y')\n",
    "  # AG\n",
    "  w_full = jax.lax.all_gather(weights, 'x', axis=0, tiled=True)\n",
    "  # jax.debug.print('w_full: {}', w_full.shape)\n",
    "  # Slice out local arrays\n",
    "  # TODO: again, fix these so they're not tied to the specific shapes\n",
    "  w_slice = jax.lax.dynamic_slice(w_full, (y_idx * 1024, 0), (1024, 1024))\n",
    "  # jax.debug.print('w_slice: {}', w_slice.shape)\n",
    "  # jax.debug.print('input_shape: {}', inputs.shape)\n",
    "  # We'll take the default tile sizes for now\n",
    "  local_out = make_matmul(inputs, w_slice)\n",
    "  # jax.debug.print('local_out: {}', local_out.shape)\n",
    "  return jax.lax.psum(local_out, 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96f9e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgk1 = jax.jit(\n",
    "    jax.shard_map(\n",
    "    distributed_gemm_kernel1,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f26d7cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.allclose(dgk1(inputs, weights), jax_matmul(inputs, weights), rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc718331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: 0.5\n",
      "mean diff: 0.05057927221059799\n",
      "median diff: 0.031246185302734375\n",
      "% > 0.1: 15.81%\n",
      "rows with errors: 2048  2048\n",
      "cols with errors: 1024 1024\n",
      "worst error at [875, 791]: ref=128, test=128.5\n",
      "top-left: mean=0.0507, % bad=15.9%\n",
      "top-right: mean=0.0506, % bad=15.8%\n",
      "bottom-left: mean=0.0505, % bad=15.8%\n",
      "bottom-right: mean=0.0505, % bad=15.7%\n"
     ]
    }
   ],
   "source": [
    "ref = jax_matmul(inputs, weights)\n",
    "test = dgk1(inputs, weights)\n",
    "diff = jnp.abs(ref - test)\n",
    "\n",
    "print(f\"max diff: {jnp.max(diff)}\")\n",
    "print(f\"mean diff: {jnp.mean(diff)}\")\n",
    "print(f\"median diff: {jnp.median(diff)}\")\n",
    "print(f\"% > 0.1: {100 * jnp.mean(diff > 0.1):.2f}%\")\n",
    "\n",
    "# Location of errors\n",
    "bad_mask = diff > 0.1\n",
    "bad_rows = jnp.any(bad_mask, axis=1)\n",
    "bad_cols = jnp.any(bad_mask, axis=0)\n",
    "print(f\"rows with errors: {jnp.sum(bad_rows)}  {ref.shape[0]}\")\n",
    "print(f\"cols with errors: {jnp.sum(bad_cols)} {ref.shape[1]}\")\n",
    "\n",
    "# Worst error location\n",
    "bad_idx = jnp.argmax(diff)\n",
    "i, j = bad_idx // ref.shape[1], bad_idx % ref.shape[1]\n",
    "print(f\"worst error at [{i}, {j}]: ref={ref[i,j]}, test={test[i,j]}\")\n",
    "\n",
    "# Check quadrants (shard boundaries)\n",
    "quadrants = [\n",
    "    (\"top-left\", diff[:1024, :512]),\n",
    "    (\"top-right\", diff[:1024, 512:]),\n",
    "    (\"bottom-left\", diff[1024:, :512]),\n",
    "    (\"bottom-right\", diff[1024:, 512:]),\n",
    "]\n",
    "for name, q in quadrants:\n",
    "    mean_val = float(jnp.mean(q))\n",
    "    pct_bad = float(100 * jnp.mean(q > 0.1))\n",
    "    print(f\"{name}: mean={mean_val:.4f}, % bad={pct_bad:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "448d42ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Running benchmark... done\n",
      "\n",
      "Results:\n",
      "  Mean:        0.013 ms\n",
      "  Median:      0.012 ms\n",
      "  Stdev:       0.006 ms\n",
      "  Min:         0.008 ms\n",
      "  P95:         0.019 ms\n",
      "  P99:         0.036 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import statistics\n",
    "\n",
    "for _ in range(3):\n",
    "    jax_matmul(inputs, weights).block_until_ready()\n",
    "print(\"done\")\n",
    "\n",
    "# Benchmark\n",
    "print(\"Running benchmark...\", end=\" \", flush=True)\n",
    "times = []\n",
    "for _ in range(50):\n",
    "    result = jax_matmul(inputs, weights)\n",
    "    result.block_until_ready()\n",
    "    start = time.perf_counter()\n",
    "    result.block_until_ready()\n",
    "    end = time.perf_counter()\n",
    "    times.append((end - start) * 1000)  # Convert to ms\n",
    "print(\"done\")\n",
    "print()\n",
    "\n",
    "# Statistics\n",
    "mean_ms = statistics.mean(times)\n",
    "median_ms = statistics.median(times)\n",
    "stdev_ms = statistics.stdev(times) if len(times) > 1 else 0\n",
    "min_ms = min(times)\n",
    "max_ms = max(times)\n",
    "p95 = np.percentile(times, 95)\n",
    "p99 = np.percentile(times, 99)\n",
    "\n",
    "print(\"Results:\")\n",
    "print(f\"  Mean:   {mean_ms:>10.3f} ms\")\n",
    "print(f\"  Median: {median_ms:>10.3f} ms\")\n",
    "print(f\"  Stdev:  {stdev_ms:>10.3f} ms\")\n",
    "print(f\"  Min:    {min_ms:>10.3f} ms\")\n",
    "print(f\"  P95:    {p95:>10.3f} ms\")\n",
    "print(f\"  P99:    {p99:>10.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3460b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think this one should have the matmul done after the full AG\n",
    "# Then we can interleave with ppermute\n",
    "\n",
    "# FIRST: Work out the AllGather\n",
    "# SECOND: Work out the MatMul\n",
    "# FINAL: Work out the AR\n",
    "\n",
    "\"\"\"\n",
    "On each local device, we have 2 arrays sitting there -> inputs + weights\n",
    "On kernel start:\n",
    "  - Barrier sync to get everyone on the same stage\n",
    "    - Might could relax this constraint?\n",
    "  - Issue remote DMAs along the ... ring to all gather the weights\n",
    "  - Once received\n",
    "    - Compute local dots\n",
    "    - Accumulate\n",
    "  - Finally\n",
    "    - All Reduce over the y ring to conform shmap shape\n",
    "\"\"\"\n",
    "\n",
    "def all_gather_kernel_1D(\n",
    "  input_ref, output_ref,\n",
    "  local_send_sem, send_sem, recv_sem, \n",
    "  # ...\n",
    "):\n",
    "  \"\"\"\n",
    "  These two refs are intended to decouple what's going on (that's the best you can do?)\n",
    "\n",
    "  The input ref is the local ref, the output ref is the ref that will be\n",
    "  sending/receiving data from our neighboring devices\n",
    "  \"\"\"\n",
    "  #TODO: Barrier\n",
    "  \n",
    "  # Get tensor dims/sizes\n",
    "  # This should be baked into compiled artifact? Or is it runtime?\n",
    "  # There has to be a prettier way to do this?\n",
    "  shard_height = input_ref.shape[0]\n",
    "  shard_width = input_ref.shape[1]\n",
    "\n",
    "  # Get neighbors\n",
    "  # Map to some position in [(0,0), (1,0), (0,1), (1,1)] along x\n",
    "  # left_dev = jax.lax.rem(device_id - 1, x_ring)\n",
    "  this_device_x = jax.lax.axis_index('x')\n",
    "  this_device_y = jax.lax.axis_index('y')\n",
    "  x_ring = jax.lax.axis_size('x')\n",
    "  y_ring = jax.lax.axis_size('y')\n",
    "  right_dev = jax.lax.rem(this_device_x + 1, x_ring)\n",
    "\n",
    "  # Hard coding 2: where 2 is supposed to be the ring_length\n",
    "  neighbor_x = (this_device_x + 1) % x_ring\n",
    "  # neighbor_linear = neighbor_x * y_ring + this_device_y\n",
    "\n",
    "  # PERFORM INITIAL ASYNC COPY FROM OUR HBM TO OUR HBM\n",
    "  # @pl.when(pl.program_id(0) == 0) -> We're just copying within our HBM to a bigger HBM memory\n",
    "  # XLA liveness should handle malloc/free the _INPUT_ tensor once the AG completes\n",
    "  #   def _copy_local_to_local \n",
    "  local_hbm_copy = pltpu.make_async_copy(\n",
    "    src_ref=input_ref,\n",
    "    dst_ref=output_ref.at[pl.ds(this_device_x * shard_height, shard_height), :],\n",
    "    sem=local_send_sem\n",
    "  )\n",
    "\n",
    "  # We can defer the wait until literally the very end of the kernel\n",
    "  local_hbm_copy.start()\n",
    "\n",
    "  # Issue RDMA\n",
    "  #NOTE: This is buggy; this won't work for axis lengths > 2\n",
    "  # Logic would keep writing to the same location in the neighbor's out_ref*\n",
    "  right_dma = pltpu.make_async_remote_copy(\n",
    "    src_ref=input_ref.at[...],\n",
    "    dst_ref=output_ref.at[pl.ds(this_device_x * shard_height, shard_height), :],\n",
    "    send_sem=send_sem,\n",
    "    recv_sem=recv_sem,\n",
    "    #NOTE: device_id has to match the mesh specs\n",
    "    # Since we're in a 2x2 grid -> Need to communication _which_ links we're using\n",
    "    # device_id=(right_dev,),\n",
    "    device_id_type=pltpu.DeviceIdType.MESH,\n",
    "    device_id=(right_dev, this_device_y),\n",
    "    # device_id=(right_dev),\n",
    "    # device_id=neighbor_linear,\n",
    "    # device_id_type=pltpu.DeviceIdType.LOGICAL,\n",
    "  )\n",
    "\n",
    "  # Wait on RDMA (send/recv)\n",
    "  right_dma.start()\n",
    "  right_dma.wait()\n",
    "  local_hbm_copy.wait()\n",
    "\n",
    "grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "  num_scalar_prefetch=0,\n",
    "  # This logic is wrong -> this is not a \"line\" of devices, but a 2x2 grid\n",
    "  # We only want to iterate along the number of devices PER RING - 1 times\n",
    "  grid=(1,), # Could we move this elsewhere?\n",
    "  in_specs=[\n",
    "    # Our input reference is just our big tensor in HBM\n",
    "    pl.BlockSpec(memory_space=pl.ANY)\n",
    "  ],\n",
    "  # Our output reference will be _another_ big tensor in HBM\n",
    "  out_specs=pl.BlockSpec(memory_space=pl.ANY),\n",
    "  # This will be an error if you need more semaphores for more neighbors\n",
    "  scratch_shapes=(\n",
    "    [pltpu.SemaphoreType.DMA] * 3\n",
    "  )\n",
    ")\n",
    "\n",
    "out_shape=jax.ShapeDtypeStruct((weights.shape), dtype=jnp.bfloat16)\n",
    "\n",
    "def make_ag(x):\n",
    "  # TODO: should we parameterize this _here_; aka pass in the shard shape data here?\n",
    "  return pl.pallas_call(\n",
    "    all_gather_kernel_1D,\n",
    "    grid_spec=grid_spec,\n",
    "    out_shape=out_shape,\n",
    "    interpret=True\n",
    "  )(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3954349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basically --\n",
    "The way we organize the grid for communications is a decision\n",
    "  - Rings, ranges, etc.\n",
    "  - This trades off bandwidth/latency\n",
    "\n",
    "\n",
    "# Ring\n",
    "grid = (ring_size - 1,)\n",
    "\n",
    "# Recursive doubling\n",
    "grid = (log2(ring_size),)\n",
    "\n",
    "# Direct\n",
    "grid = (1,)  # but more complex RDMA pattern\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bb445c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xla_allgather(weights):\n",
    "    w_full = jax.lax.all_gather(weights, 'x', axis=0, tiled=True)\n",
    "    return w_full\n",
    "\n",
    "res = jax.jit(\n",
    "    jax.shard_map(\n",
    "        xla_allgather,\n",
    "        mesh=mesh,\n",
    "        in_specs=P('x', None),\n",
    "        out_specs=P(),\n",
    "        check_vma=False\n",
    "    )\n",
    ")(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e710440e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When `check_vma=True` on `jax.shard_map`, `vma` on `jax.ShapeDtypeStruct` must not be `None`. Please specify how the output should be varying across mesh axes using the `vma` argument of `jax.ShapeDtypeStruct` or set `check_vma=False` on `jax.shard_map`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res2 = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshard_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmake_ag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43min_specs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mP\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout_specs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mP\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# check_vma=False\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 26 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mmake_ag\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_ag\u001b[39m(x):\n\u001b[32m    109\u001b[39m   \u001b[38;5;66;03m# TODO: should we parameterize this _here_; aka pass in the shard shape data here?\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpallas_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_gather_kernel_1D\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrid_spec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterpret\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    115\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 12 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/jax/_src/pallas/pallas_call.py:1499\u001b[39m, in \u001b[36m_pallas_call.<locals>.wrapped\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m   1496\u001b[39m in_paths, flat_args = unzip2(flat_args_with_paths)\n\u001b[32m   1497\u001b[39m flat_in_avals = \u001b[38;5;28mtuple\u001b[39m(jax_core.get_aval(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m flat_args)\n\u001b[32m-> \u001b[39m\u001b[32m1499\u001b[39m flat_out_avals = \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert_out_shape_to_aval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1500\u001b[39m \u001b[43m                       \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mflat_out_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m in_origins = \u001b[38;5;28mtuple\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtree_util.keystr(p)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m in_paths)\n\u001b[32m   1503\u001b[39m out_origins = \u001b[38;5;28mtuple\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtree_util.keystr(p)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m out_paths)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/jax/_src/pallas/pallas_call.py:1499\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1496\u001b[39m in_paths, flat_args = unzip2(flat_args_with_paths)\n\u001b[32m   1497\u001b[39m flat_in_avals = \u001b[38;5;28mtuple\u001b[39m(jax_core.get_aval(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m flat_args)\n\u001b[32m-> \u001b[39m\u001b[32m1499\u001b[39m flat_out_avals = \u001b[38;5;28mtuple\u001b[39m(\u001b[43m_convert_out_shape_to_aval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1500\u001b[39m                        \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m flat_out_shapes)\n\u001b[32m   1502\u001b[39m in_origins = \u001b[38;5;28mtuple\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtree_util.keystr(p)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m in_paths)\n\u001b[32m   1503\u001b[39m out_origins = \u001b[38;5;28mtuple\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtree_util.keystr(p)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m out_paths)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/sfp/.venv/lib/python3.13/site-packages/jax/_src/pallas/pallas_call.py:1150\u001b[39m, in \u001b[36m_convert_out_shape_to_aval\u001b[39m\u001b[34m(out_shape)\u001b[39m\n\u001b[32m   1148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config._check_vma.value:\n\u001b[32m   1149\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m out_shape.vma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1151\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhen `check_vma=True` on `jax.shard_map`, `vma` on\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1152\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `jax.ShapeDtypeStruct` must not be `None`. Please specify how the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1153\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m output should be varying across mesh axes using the `vma`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1154\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m argument of `jax.ShapeDtypeStruct` or set `check_vma=False` on\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `jax.shard_map`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1156\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m jax_core.ShapedArray(\n\u001b[32m   1157\u001b[39m       shape=out_shape.shape, dtype=out_shape.dtype,\n\u001b[32m   1158\u001b[39m       sharding=jax_core.get_cur_mesh_sharding(), vma=out_shape.vma)\n\u001b[32m   1159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jax_core.ShapedArray(shape=out_shape.shape, dtype=out_shape.dtype,\n\u001b[32m   1160\u001b[39m                             sharding=jax_core.get_cur_mesh_sharding())\n",
      "\u001b[31mValueError\u001b[39m: When `check_vma=True` on `jax.shard_map`, `vma` on `jax.ShapeDtypeStruct` must not be `None`. Please specify how the output should be varying across mesh axes using the `vma` argument of `jax.ShapeDtypeStruct` or set `check_vma=False` on `jax.shard_map`."
     ]
    }
   ],
   "source": [
    "res2 = jax.jit(\n",
    "    jax.shard_map(\n",
    "        make_ag,\n",
    "        mesh=mesh,\n",
    "        in_specs=P('x', None),\n",
    "        out_specs=P(None, None),\n",
    "        # check_vma=False\n",
    "    )\n",
    ")(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc015c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here's something fun --\n",
    "\n",
    "NotImplementedError: Meshes with more than 1 named\n",
    "dimension not implemented in dma_start_p\n",
    "\n",
    "Pallas's remote DMA primitives currently only support 1D meshes. That's a real limitation.\n",
    "\n",
    "https://github.com/jax-ml/jax/blob/main/jax/_src/pallas/mosaic/primitives.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_shmap_ag_kernel(\n",
    "    input_ref, output_ref,\n",
    "    local_copy_sem, send_sem, recv_sem\n",
    "):\n",
    "  # Get axis detail\n",
    "  # Make Local HBM <--> HBM copy\n",
    "  # Make Remote HBM <--> HBM copy\n",
    "\n",
    "  pass\n",
    "\n",
    "# def no_shmap_ag(input):\n",
    "#     return pl.pallas_call(\n",
    "#         kernel,\n",
    "#         grid_spec=(),\n",
    "#         out_shape=(),\n",
    "        \n",
    "#     )(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_gather_kernel_2D(\n",
    "  x\n",
    "):\n",
    "  # Barrier\n",
    "  \n",
    "  # Get neighbors\n",
    "\n",
    "  # Issue RDMA\n",
    "\n",
    "  # Wait on RDMA (send/recv)\n",
    "  # NOTE: need to change the semaphores/refs to handle reads/writes from multiple neighbors\n",
    "  # NOTE: In a 2x2 grid, there's only one neighbor**\n",
    "  # Does this need to be accounted for in code? -> For loop type construct to handle this  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09deb5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_all_gather():\n",
    "    pass\n",
    "\n",
    "\n",
    "res = jax.jit(\n",
    "    jax.shard_map(\n",
    "        new_all_gather,\n",
    "        mesh=mesh,\n",
    "        in_specs=(P(), P()),\n",
    "        out_specs=P(),\n",
    "        # check_vma=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.shard_map(\n",
    "#     ...\n",
    "# )(inputs, weights)\n",
    "\n",
    "# emit_pipeline version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3365c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUT IT TOGETHER\n",
    "# INTERLEAVE THE COMPUTE with ppermute\n",
    "# emit_pipeline??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb556444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMERICS CHECK\n",
    "\n",
    "# ACCURACY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c628a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROFILING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTEND THE KERNEL TO BE MORE GENERAL THAN 2x2\n",
    "# REPEAT CHECKS\n",
    "# SCALE UP TO 4x4 GRID"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
