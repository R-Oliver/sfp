{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18877895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "\n",
    "import jax\n",
    "from jax.experimental import pallas as pl\n",
    "from jax.experimental.pallas import tpu as pltpu\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import NamedSharding, PartitionSpec as P\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b284a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update('jax_num_cpu_devices', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a5da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "def project_root() -> Path:\n",
    "    return Path(subprocess.check_output(\n",
    "        ['git', 'rev-parse', '--show-toplevel']\n",
    "    ).decode().strip())\n",
    "\n",
    "TRACES_DIR = project_root() / \"traces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79eb75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, k, n = 2048, 2048, 1024\n",
    "\n",
    "k1, k2 = jax.random.split(jax.random.key(0), 2)\n",
    "inputs = jax.random.normal(k1, (m, k), dtype=jnp.bfloat16)\n",
    "weights = jax.random.normal(k2, (k, n), dtype=jnp.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b67d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/my/845wytg53ln_99j7k_dzpl840000gn/T/ipykernel_74221/3396536806.py:1: DeprecationWarning: The default axis_types will change in JAX v0.9.0 to jax.sharding.AxisType.Explicit. To maintain the old behavior, pass `axis_types=(jax.sharding.AxisType.Auto,) * len(axis_names)`. To opt-into the new behavior, pass `axis_types=(jax.sharding.AxisType.Explicit,) * len(axis_names)\n",
      "  mesh = jax.make_mesh((2, 2), (\"x\", \"y\"))\n"
     ]
    }
   ],
   "source": [
    "mesh = jax.make_mesh((2, 2), (\"x\", \"y\"))\n",
    "inp_sharding = jax.NamedSharding(mesh, P('x', 'y'))\n",
    "w_sharding = jax.NamedSharding(mesh, P('x', None))\n",
    "\n",
    "inputs = jax.device_put(inputs, inp_sharding)\n",
    "weights = jax.device_put(weights, w_sharding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c93d6",
   "metadata": {},
   "source": [
    "inputs are size 2048, 2048 -> bf16:: 2 bytes * 2048 * 2048 = ~8MB\n",
    "weights are size 2048, 1024 -> bf16:: 2 bytes * 2048 * 1024 = ~4MB\n",
    "\n",
    "inputs sharded along x and y -> $Inp[I_{X}, J_{Y}]$\n",
    "\n",
    "weights sharded along x -> $W[J_{X}, K]$\n",
    "\n",
    "Each device has N elements per array:\n",
    "  - inputs\n",
    "    - (2048 / 2) * (2048 / 2) * 2bytes\n",
    "    - ~2MB\n",
    "  - weights\n",
    "    - (2048 / 2) * 1024 * 2bytes\n",
    "    - ~2MB\n",
    "\n",
    "The contracting dimension is sharded in both inputs and weights, along different axes.\n",
    "Need to handle that with collectives; AG/AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e11eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   CPU 0    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   CPU 1    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">   CPU 2    </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">   CPU 3    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m   \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mCPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m    \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m   \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mCPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ea6c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0,1   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 2,3   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0,1\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 2,3\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eae0f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_matmul(x: jax.Array, y: jax.Array) -> jax.Array:\n",
    "    return jnp.matmul(x, y)\n",
    "\n",
    "out = basic_matmul(inputs, weights)\n",
    "compiled = jax.jit(basic_matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5a9fdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0,1   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 2,3   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0,1\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 2,3\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37b47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compiled(inputs, weights)\n",
    "result.block_until_ready()\n",
    "\n",
    "with jax.profiler.trace(TRACES_DIR):\n",
    "    result = compiled(inputs, weights)\n",
    "    result.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa631a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.shard_map,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")\n",
    "def xla_matmul(input_shard: jax.Array, w_shard: jax.Array) -> jax.Array:\n",
    "    # First we want to all_gather the data\n",
    "    with jax.named_scope('all_gather(s)'):\n",
    "        input_full = jax.lax.all_gather(input_shard, 'y', axis=1, tiled=True)\n",
    "        w_full = jax.lax.all_gather(w_shard, 'x', axis=0, tiled=True) # gather w along x\n",
    "    # Then we want to compute on the data\n",
    "    with jax.named_scope('dot'):\n",
    "        local_out = input_full @ w_full\n",
    "    # Then we want to all reduce the data\n",
    "    # with jax.named_scope('all_reduce'):\n",
    "    #     out = jax.lax.psum(local_out, 'y')\n",
    "    return local_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "950f9651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://docs.jax.dev/en/latest/notebooks/shard_map.html\n",
    "from jax.tree_util import tree_map, tree_all\n",
    "\n",
    "def allclose(a, b):\n",
    "  return tree_all(tree_map(functools.partial(jnp.allclose, atol=1e-2, rtol=1e-2), a, b))\n",
    "\n",
    "allclose(xla_matmul(inputs, weights), jnp.dot(inputs, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0e27b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemm2_compiled = jax.jit(xla_matmul)\n",
    "result = gemm2_compiled(inputs, weights)\n",
    "result.block_until_ready()\n",
    "\n",
    "with jax.profiler.trace(TRACES_DIR):\n",
    "    result = gemm2_compiled(inputs, weights)\n",
    "    result.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8035049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.shard_map,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")\n",
    "def xla_matmul2(input_shard: jax.Array, weight_shard: jax.Array) -> jax.Array:\n",
    "    \"\"\"\n",
    "    This time, we want to make the computation a little more efficient than\n",
    "    stacking the two all gathers are the beginning of the kernel\n",
    "    \"\"\"\n",
    "    y_idx = jax.lax.axis_index('y')\n",
    "    # All gather the weights over x so that each device contains full copy\n",
    "    w_full = jax.lax.all_gather(weight_shard, 'x', axis=0, tiled=True)\n",
    "    # Using the y-ring axis to determined which col stripe of weights to compute locally\n",
    "    w_slice = jax.lax.dynamic_slice(w_full, (y_idx * 1024, 0), (1024, 1024))\n",
    "    local_out = input_shard @ w_slice\n",
    "    # All Reduce over the y-ring to accumulate partial results\n",
    "    out = jax.lax.psum(local_out, 'y')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0113ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = xla_matmul2(inputs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1629ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allclose(basic_matmul(inputs, weights), xla_matmul2(inputs, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8d0812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.shard_map,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")\n",
    "def xla_matmul3(input_shard: jax.Array, weight_shard: jax.Array) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Use some higher precision numerics to accomodate accumulation order\n",
    "    \"\"\"\n",
    "    y_idx = jax.lax.axis_index('y')\n",
    "    w_full = jax.lax.all_gather(weight_shard, 'x', axis=0, tiled=True)\n",
    "    # This shouldn't hardcode the dim shapes\n",
    "    w_slice = jax.lax.dynamic_slice(w_full, (y_idx * 1024, 0), (1024, 1024))\n",
    "    # This is probably overkill, think it might also incur perf penalty\n",
    "    #   - Something in the docs about precision highest\n",
    "    local_out = jax.lax.dot_general(\n",
    "        input_shard, w_slice,\n",
    "        dimension_numbers=(((1,), (0,)), ((), ())),\n",
    "        precision=jax.lax.Precision.HIGHEST,\n",
    "        preferred_element_type=jnp.float32,\n",
    "    )\n",
    "    out = jax.lax.psum(local_out, 'y')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39c84ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.allclose(xla_matmul3(inputs, weights), basic_matmul(inputs, weights), rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578eb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's recall what we've learned so far --\n",
    "\n",
    "When needed to perform an all gather on the reduction axis of our weights\n",
    "to remove the sharding over X\n",
    "\n",
    "Then, we compute local MatMuls (slicing out the appropriate data) between\n",
    "the shard local inputs and the full weights\n",
    "- Recall, we have the full weights after AG, so we need to slice out the\n",
    "  appropriate chunks of W for our computation\n",
    "\n",
    "These MatMuls are accumulators -> They finally need to be all reduced\n",
    "over Y. The desired out sharding is ('x', None), so when we do an AR\n",
    "over the Y axis, we are sharing the partial results\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Let's sketch out the algorithm we care about --\n",
    "\n",
    "We have 2 arrays distributed over 4 devices\n",
    "  - 1/4 of inputs on each device\n",
    "  - 1/2 of weights of each device\n",
    "\n",
    "We want to efficiently compute this distributed matmul over the devices\n",
    "\n",
    "We know that the contracting dims are sharded differently\n",
    "\n",
    "So there will need to be some comms to unshard so that we have the\n",
    "whole array in the right place. HOWEVER, we may also be able to get\n",
    "away with ppermute to simple pass _results_ after compute is finished\n",
    "\n",
    "Here are the kernels we may want to try\n",
    "  - Simple matmul with lax collectives inserted in the right spots\n",
    "  - MatMul with handrolled collectives (still AG to start, then AR)\n",
    "  - ppermute\n",
    "    - Issue async DMA\n",
    "    - Run local compute; stash in accumulator\n",
    "    - \n",
    "    - How much latency can we hide here?\n",
    "      - If the DMAs are fast/slow?\n",
    "      - How do we _reason_ about these tradeoffs\n",
    "\n",
    "Start with 2x2 case; don't worry too much about abstracting things out\n",
    "  - Then extend to larger configurations + abstractions\n",
    "  - How do we think about the work we're doing?\n",
    "  - Where are the opportunities to show different edge cases?\n",
    "  - Where do our assumptions break down?\n",
    "  - emit_pipeline\n",
    "  - kernel schedule?\n",
    "  - What happens when we run this on Trillium?\n",
    "    - What changes?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb35ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here's what happens:\n",
    "- We have 2 arrays in HBM\n",
    "- Need to all_gather weights over x, now each device has fully copy of x\n",
    "- Then do local compute\n",
    "- All Reduce the local compute to get the correct results\n",
    "\"\"\"\n",
    "\n",
    "#NOTE: THIS IS THE CASE WHERE WE SIMPLY REPLACE jax.dot/jax.matmul/x @ y in xla_matmul3\n",
    "def simple_matmul(x_ref, y_ref, o_ref, scratch_ref, *, n_steps):\n",
    "  # Zero scratch buffer\n",
    "  @pl.when(pl.program_id(2) == 0)\n",
    "  def _init_scratch():\n",
    "    scratch_ref[...] = jnp.zeros_like(scratch_ref)\n",
    "\n",
    "  # Compute dot\n",
    "  scratch_ref[...] += jnp.dot(\n",
    "    x_ref[...],\n",
    "    y_ref[...],\n",
    "    preferred_element_type=jnp.float32\n",
    "  )\n",
    "\n",
    "  # Flush to HBM\n",
    "  @pl.when(pl.program_id(2) == n_steps - 1)\n",
    "  def _flush_scratch():\n",
    "    o_ref[...] = scratch_ref[...].astype(o_ref.dtype)\n",
    "\n",
    "\n",
    "def make_matmul(\n",
    "  x: jax.Array,\n",
    "  y: jax.Array,\n",
    "  *,\n",
    "  bm: int = 128,\n",
    "  bk: int = 128,\n",
    "  bn: int = 128,\n",
    "):\n",
    "  m, k = x.shape\n",
    "  _, n = y.shape\n",
    "\n",
    "  grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "    num_scalar_prefetch=0,\n",
    "    grid=(m//bm, n//bn, k//bk),\n",
    "    in_specs=[\n",
    "      pl.BlockSpec((bm, bk), lambda i,j,k: (i, k)),\n",
    "      pl.BlockSpec((bk, bn), lambda i,j,k: (k, j))\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),\n",
    "    scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)]\n",
    "  )\n",
    "\n",
    "  return pl.pallas_call(\n",
    "    functools.partial(simple_matmul, n_steps=k//bk),\n",
    "    grid_spec=grid_spec,\n",
    "    # Made this float32 to appease the numerics gods\n",
    "    out_shape=jax.ShapeDtypeStruct((m, n), dtype=jnp.float32),\n",
    "    interpret=True\n",
    "  )(x, y)\n",
    "\n",
    "\n",
    "def distributed_gemm_kernel1(inputs, weights):\n",
    "  y_idx = jax.lax.axis_index('y')\n",
    "  # AG\n",
    "  w_full = jax.lax.all_gather(weights, 'x', axis=0, tiled=True)\n",
    "  jax.debug.print('w_full: {}', w_full.shape)\n",
    "  # Slice out local arrays\n",
    "  # TODO: again, fix these so they're not tied to the specific shapes\n",
    "  w_slice = jax.lax.dynamic_slice(w_full, (y_idx * 1024, 0), (1024, 1024))\n",
    "  jax.debug.print('w_slice: {}', w_slice.shape)\n",
    "  jax.debug.print('input_shape: {}', inputs.shape)\n",
    "  # We'll take the default tile sizes for now\n",
    "  local_out = make_matmul(inputs, w_slice)\n",
    "  jax.debug.print('local_out: {}', local_out.shape)\n",
    "  return jax.lax.psum(local_out, 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "96f9e8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_full: (Array(2048, dtype=int32), Array(1024, dtype=int32))\n",
      "w_full: (Array(2048, dtype=int32), Array(1024, dtype=int32))\n",
      "w_full: (Array(2048, dtype=int32), Array(1024, dtype=int32))\n",
      "w_full: (Array(2048, dtype=int32), Array(1024, dtype=int32))\n",
      "w_slice: (Array(1024, dtype=int32), Array(1024, dtype=int32))\n",
      "w_slice: (Array(1024, dtype=int32), Array(1024, dtype=int32))\n",
      "w_slice: (Array(1024, dtype=int32), Array(1024, dtype=int32))\n",
      "w_slice: (Array(1024, dtype=int32), Array(1024, dtype=int32))\n",
      "input_shape: (Array(1024, dtype=int32), Array(1024, dtype=int32))\n",
      "input_shape: (Array(1024, dtype=int32), Array(1024, dtype=int32))\n",
      "input_shape: (Array(1024, dtype=int32), Array(1024, dtype=int32))\n",
      "input_shape: (Array(1024, dtype=int32), Array(1024, dtype=int32))\n",
      "local_out: (Array(1024, dtype=int32), Array(1024, dtype=int32))\n",
      "local_out: (Array(1024, dtype=int32), Array(1024, dtype=int32))\n",
      "local_out: (Array(1024, dtype=int32), Array(1024, dtype=int32))\n",
      "local_out: (Array(1024, dtype=int32), Array(1024, dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "dgk1 = jax.shard_map(\n",
    "    distributed_gemm_kernel1,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")(inputs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f26d7cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.allclose(dgk1, basic_matmul(inputs, weights), rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc718331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: 0.5\n",
      "mean diff: 0.05057927221059799\n",
      "median diff: 0.031246185302734375\n",
      "% > 0.1: 15.81%\n",
      "rows with errors: 2048  2048\n",
      "cols with errors: 1024 1024\n",
      "worst error at [875, 791]: ref=128, test=128.5\n",
      "top-left: mean=0.0507, % bad=15.9%\n",
      "top-right: mean=0.0506, % bad=15.8%\n",
      "bottom-left: mean=0.0505, % bad=15.8%\n",
      "bottom-right: mean=0.0505, % bad=15.7%\n"
     ]
    }
   ],
   "source": [
    "ref = basic_matmul(inputs, weights)\n",
    "test = dgk1\n",
    "diff = jnp.abs(ref - test)\n",
    "\n",
    "print(f\"max diff: {jnp.max(diff)}\")\n",
    "print(f\"mean diff: {jnp.mean(diff)}\")\n",
    "print(f\"median diff: {jnp.median(diff)}\")\n",
    "print(f\"% > 0.1: {100 * jnp.mean(diff > 0.1):.2f}%\")\n",
    "\n",
    "# Location of errors\n",
    "bad_mask = diff > 0.1\n",
    "bad_rows = jnp.any(bad_mask, axis=1)\n",
    "bad_cols = jnp.any(bad_mask, axis=0)\n",
    "print(f\"rows with errors: {jnp.sum(bad_rows)}  {ref.shape[0]}\")\n",
    "print(f\"cols with errors: {jnp.sum(bad_cols)} {ref.shape[1]}\")\n",
    "\n",
    "# Worst error location\n",
    "bad_idx = jnp.argmax(diff)\n",
    "i, j = bad_idx // ref.shape[1], bad_idx % ref.shape[1]\n",
    "print(f\"worst error at [{i}, {j}]: ref={ref[i,j]}, test={test[i,j]}\")\n",
    "\n",
    "# Check quadrants (shard boundaries)\n",
    "quadrants = [\n",
    "    (\"top-left\", diff[:1024, :512]),\n",
    "    (\"top-right\", diff[:1024, 512:]),\n",
    "    (\"bottom-left\", diff[1024:, :512]),\n",
    "    (\"bottom-right\", diff[1024:, 512:]),\n",
    "]\n",
    "for name, q in quadrants:\n",
    "    mean_val = float(jnp.mean(q))\n",
    "    pct_bad = float(100 * jnp.mean(q > 0.1))\n",
    "    print(f\"{name}: mean={mean_val:.4f}, % bad={pct_bad:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may not be a part of the \"build it up\" story quite yet...\n",
    "# May need one more kernel to handle things\n",
    "def matmul_kernel_pipelining(x_ref, y_ref, o_ref):\n",
    "  # Prologue\n",
    "\n",
    "  # Need to make the iteration space generic over axis sizes\n",
    "  # pl.program_id(...) -> The AGs/ARs will run N-1 times\n",
    "\n",
    "  # Compute\n",
    "  # Should probably have an inner pipeline schedule for the GEMM\n",
    "  # I think that this should work?\n",
    "  # emit_pipeline(...)\n",
    "\n",
    "  # Epilogue\n",
    "\n",
    "  pass\n",
    "\n",
    "# Grid should be N-1 times?\n",
    "# Or... (Grid - 1, Grid - 1)\n",
    "# This way we have the initial AllGather\n",
    "# There is an inner pipeline per iteration\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "def matmul_kernel_async():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb556444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMERICS CHECK\n",
    "\n",
    "# ACCURACY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c628a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROFILING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTEND THE KERNEL TO BE MORE GENERAL THAN 2x2\n",
    "# REPEAT CHECKS\n",
    "# SCALE UP TO 4x4 GRID"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
