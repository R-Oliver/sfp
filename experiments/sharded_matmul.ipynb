{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18877895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import jax\n",
    "from jax.experimental import pallas as pl\n",
    "from jax.experimental.pallas import tpu as pltpu\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import NamedSharding, PartitionSpec as P\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sfp.utils import benchmark, numerics, profile, upload_to_gcs\n",
    "\n",
    "jax.config.update('jax_num_cpu_devices', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, k, n = 2048, 2048, 1024\n",
    "\n",
    "k1, k2 = jax.random.split(jax.random.key(0), 2)\n",
    "inputs = jax.random.normal(k1, (m, k), dtype=jnp.bfloat16)\n",
    "weights = jax.random.normal(k2, (k, n), dtype=jnp.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47934/3124851191.py:2: DeprecationWarning: The default axis_types will change in JAX v0.9.0 to jax.sharding.AxisType.Explicit. To maintain the old behavior, pass `axis_types=(jax.sharding.AxisType.Auto,) * len(axis_names)`. To opt-into the new behavior, pass `axis_types=(jax.sharding.AxisType.Explicit,) * len(axis_names)\n",
      "  mesh = jax.make_mesh((2, 2), (\"x\", \"y\")) # (jax.sharding.AxisType.Explicit, jax.sharding.AxisType.Explicit)\n"
     ]
    }
   ],
   "source": [
    "num_devices = jax.device_count()\n",
    "mesh = jax.make_mesh((2, 2), (\"x\", \"y\"))\n",
    "inp_sharding = NamedSharding(mesh, P('x', 'y'))\n",
    "w_sharding = NamedSharding(mesh, P('x', None))\n",
    "\n",
    "inputs = jax.device_put(inputs, inp_sharding)\n",
    "weights = jax.device_put(weights, w_sharding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c93d6",
   "metadata": {},
   "source": [
    "inputs are size 2048, 2048 -> bf16:: 2 bytes * 2048 * 2048 = ~8MB\n",
    "weights are size 2048, 1024 -> bf16:: 2 bytes * 2048 * 1024 = ~4MB\n",
    "\n",
    "inputs sharded along x and y -> $Inp[I_{X}, J_{Y}]$\n",
    "\n",
    "weights sharded along x -> $W[J_{X}, K]$\n",
    "\n",
    "Each device has N elements per array:\n",
    "  - inputs\n",
    "    - (2048 / 2) * (2048 / 2) * 2bytes\n",
    "    - ~2MB\n",
    "  - weights\n",
    "    - (2048 / 2) * 1024 * 2bytes\n",
    "    - ~2MB\n",
    "\n",
    "The contracting dimension is sharded in both inputs and weights, along different axes.\n",
    "Need to handle that with collectives; AG/AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81e11eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   TPU 0    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   TPU 1    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">   TPU 3    </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">   TPU 2    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m   \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 3\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m    \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m   \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 2\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ea6c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  TPU 0,1   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  TPU 2,3   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0,1\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 2,3\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa631a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_matmul(x: jax.Array, y: jax.Array) -> jax.Array:\n",
    "    return jnp.matmul(x, y)\n",
    "\n",
    "@functools.partial(\n",
    "    jax.shard_map,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")\n",
    "def xla_matmul_1(input_shard: jax.Array, w_shard: jax.Array) -> jax.Array:\n",
    "    # First we want to all_gather the data\n",
    "    with jax.named_scope('all_gather(s)'):\n",
    "        input_full = jax.lax.all_gather(input_shard, 'y', axis=1, tiled=True)\n",
    "        w_full = jax.lax.all_gather(w_shard, 'x', axis=0, tiled=True) # gather w along x\n",
    "    # Then we want to compute on the data\n",
    "    # Since we did two full all gathers to start, regular GEMMs\n",
    "    with jax.named_scope('dot'):\n",
    "        local_out = input_full @ w_full\n",
    "    return local_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "627ed918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkResult (50 iters, 3 warmup)\n",
       "  mean:        0.306 ms\n",
       "  median:      0.302 ms\n",
       "  stdev:       0.014 ms\n",
       "  min:         0.289 ms\n",
       "  max:         0.358 ms\n",
       "  p95:         0.331 ms\n",
       "  p99:         0.352 ms"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numerics.compare(jax_matmul(inputs, weights), xla_matmul_1(inputs, weights), rtol=1e-2, atol=1e-2, region_grid=(2,2))\n",
    "jitted = jax.jit(xla_matmul_1)\n",
    "benchmark(jitted, inputs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "950f9651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  TPU 0,1   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  TPU 2,3   </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0,1\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 2,3\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = jax_matmul(inputs, weights)\n",
    "jax.debug.visualize_array_sharding(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e27b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    %all-reduce = bf16[1024,1024]{1,0:T(8,128)(2,1)} all-reduce(bf16[1024,1024]{1,0:T(8,128)(2,1)S(1)} %fusion), channel_id=2,\\n    replica_groups=[2,2]<=[4], use_global_device_ids=true, to_apply=%add.clone\\n\\n    ^^ {1,0:T(8,128)(2,1)} ; column major -- probably RHS prepping for tranpose\\n\\n    %fusion = bf16[1024,1024]{1,0:T(8,128)(2,1)S(1)} fusion(bf16[1024,1024]{1,0:T(8,128)(2,1)S(1)} %copy-done,\\n    bf16[1024,1024]{1,0:T(8,128)(2,1)S(1)} %collective-permute-done), kind=kOutput, calls=%fused_computation\\n\\n    %all-reduce = bf16[1024,1024]{1,0:T(8,128)(2,1)} all-reduce(bf16[1024,1024]{1,0:T(8,128)(2,1)S(1)} %fusion), channel_id=2,\\n    replica_groups=[2,2]<=[4], use_global_device_ids=true, to_apply=%add.clone\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax_matmul_compiled = jax.jit(jax_matmul)\n",
    "jmc = jax_matmul_compiled(inputs, weights)\n",
    "jmc.block_until_ready()\n",
    "\n",
    "with jax.profiler.trace('./traces'):\n",
    "    result = jax_matmul_compiled(inputs, weights)\n",
    "    result.block_until_ready()\n",
    "\n",
    "\"\"\"\n",
    "    %all-reduce = bf16[1024,1024]{1,0:T(8,128)(2,1)} all-reduce(bf16[1024,1024]{1,0:T(8,128)(2,1)S(1)} %fusion), channel_id=2,\n",
    "    replica_groups=[2,2]<=[4], use_global_device_ids=true, to_apply=%add.clone\n",
    "\n",
    "    ^^ {1,0:T(8,128)(2,1)} ; column major -- probably RHS prepping for tranpose\n",
    "\n",
    "    %fusion = bf16[1024,1024]{1,0:T(8,128)(2,1)S(1)} fusion(bf16[1024,1024]{1,0:T(8,128)(2,1)S(1)} %copy-done,\n",
    "    bf16[1024,1024]{1,0:T(8,128)(2,1)S(1)} %collective-permute-done), kind=kOutput, calls=%fused_computation\n",
    "\n",
    "    %all-reduce = bf16[1024,1024]{1,0:T(8,128)(2,1)} all-reduce(bf16[1024,1024]{1,0:T(8,128)(2,1)S(1)} %fusion), channel_id=2,\n",
    "    replica_groups=[2,2]<=[4], use_global_device_ids=true, to_apply=%add.clone\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# xm1 = jax.jit(xla_matmul_1)\n",
    "# result = xm1(inputs, weights)\n",
    "# result.block_until_ready()\n",
    "\n",
    "# with jax.profiler.trace(TRACES_DIR):\n",
    "#     result = xm1(inputs, weights)\n",
    "#     result.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8035049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.shard_map,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")\n",
    "def xla_matmul_2(input_shard: jax.Array, weight_shard: jax.Array) -> jax.Array:\n",
    "    \"\"\"\n",
    "    This time, we want to make the computation a little more efficient than\n",
    "    stacking the two all gathers at the beginning of the kernel\n",
    "\n",
    "    All Reduce at the end over partial sums\n",
    "    \"\"\"\n",
    "    y_idx = jax.lax.axis_index('y')\n",
    "    # All gather the weights over x so that each device contains full copy\n",
    "    w_full = jax.lax.all_gather(weight_shard, 'x', axis=0, tiled=True)\n",
    "    # Using the y-ring axis to determined which col stripe of weights to compute locally\n",
    "    w_slice = jax.lax.dynamic_slice(w_full, (y_idx * 1024, 0), (1024, 1024))\n",
    "    local_out = input_shard @ w_slice\n",
    "    # All Reduce over the y-ring to accumulate partial results\n",
    "    out = jax.lax.psum(local_out, 'y')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0113ffbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.allclose(jax_matmul(inputs, weights), xla_matmul_2(inputs, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8d0812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.shard_map,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")\n",
    "def xla_matmul_3(input_shard: jax.Array, weight_shard: jax.Array) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Use some higher precision numerics to demonstrate accumulation order (fp32)\n",
    "    \"\"\"\n",
    "    y_idx = jax.lax.axis_index('y')\n",
    "    w_full = jax.lax.all_gather(weight_shard, 'x', axis=0, tiled=True)\n",
    "    # This shouldn't hardcode the dim shapes\n",
    "    w_slice = jax.lax.dynamic_slice(w_full, (y_idx * 1024, 0), (1024, 1024))\n",
    "    # This is probably overkill, think it might also incur perf penalty\n",
    "    #   - Something in the docs about precision highest\n",
    "    local_out = jax.lax.dot_general(\n",
    "        input_shard, w_slice,\n",
    "        dimension_numbers=(((1,), (0,)), ((), ())),\n",
    "        precision=jax.lax.Precision.HIGHEST,\n",
    "        preferred_element_type=jnp.float32,\n",
    "    )\n",
    "    out = jax.lax.psum(local_out, 'y')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39c84ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(False, dtype=bool)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.allclose(xla_matmul_3(inputs, weights), jax_matmul(inputs, weights), rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578eb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's recall what we've learned so far --\n",
    "\n",
    "When needed to perform an all gather on the reduction axis of our weights\n",
    "to remove the sharding over X\n",
    "\n",
    "Then, we compute local MatMuls (slicing out the appropriate data) between\n",
    "the shard local inputs and the full weights\n",
    "- Recall, we have the full weights after AG, so we need to slice out the\n",
    "  appropriate chunks of W for our computation\n",
    "\n",
    "These MatMuls are accumulators -> They finally need to be all reduced\n",
    "over Y. The desired out sharding is ('x', None), so when we do an AR\n",
    "over the Y axis, we are sharing the partial results\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Let's sketch out the algorithm we care about --\n",
    "\n",
    "We have 2 arrays distributed over 4 devices\n",
    "  - 1/4 of inputs on each device\n",
    "  - 1/2 of weights of each device\n",
    "\n",
    "We want to efficiently compute this distributed matmul over the devices\n",
    "\n",
    "We know that the contracting dims are sharded differently\n",
    "\n",
    "So there will need to be some comms to unshard so that we have the\n",
    "whole array in the right place. HOWEVER, we may also be able to get\n",
    "away with ppermute to simple pass _results_ after compute is finished\n",
    "\n",
    "Here are the kernels we may want to try\n",
    "  - Simple matmul with lax collectives inserted in the right spots\n",
    "  - MatMul with handrolled collectives (still AG to start, then AR)\n",
    "  - ppermute\n",
    "    - Issue async DMA\n",
    "    - Run local compute; stash in accumulator\n",
    "    - \n",
    "    - How much latency can we hide here?\n",
    "      - If the DMAs are fast/slow?\n",
    "      - How do we _reason_ about these tradeoffs\n",
    "\n",
    "Start with 2x2 case; don't worry too much about abstracting things out\n",
    "  - Then extend to larger configurations + abstractions\n",
    "  - How do we think about the work we're doing?\n",
    "  - Where are the opportunities to show different edge cases?\n",
    "  - Where do our assumptions break down?\n",
    "  - emit_pipeline\n",
    "  - kernel schedule?\n",
    "  - What happens when we run this on Trillium?\n",
    "    - What changes?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "edb35ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here's what happens:\n",
    "- We have 2 arrays in HBM\n",
    "- Need to all_gather weights over x, now each device has fully copy of x\n",
    "- Then do local compute\n",
    "- All Reduce the local compute to get the correct results\n",
    "\"\"\"\n",
    "\n",
    "#NOTE: THIS IS THE CASE WHERE WE SIMPLY REPLACE jax.dot/jax.matmul/x @ y in xla_matmul3\n",
    "def simple_matmul(x_ref, y_ref, o_ref, scratch_ref, *, n_steps):\n",
    "  # Zero scratch buffer\n",
    "  @pl.when(pl.program_id(2) == 0)\n",
    "  def _init_scratch():\n",
    "    scratch_ref[...] = jnp.zeros_like(scratch_ref)\n",
    "\n",
    "  # Compute dot\n",
    "  scratch_ref[...] += jnp.dot(\n",
    "    x_ref[...],\n",
    "    y_ref[...],\n",
    "    preferred_element_type=jnp.float32\n",
    "  )\n",
    "\n",
    "  # Flush to HBM\n",
    "  @pl.when(pl.program_id(2) == n_steps - 1)\n",
    "  def _flush_scratch():\n",
    "    o_ref[...] = scratch_ref[...].astype(o_ref.dtype)\n",
    "\n",
    "\n",
    "def make_matmul(\n",
    "  x: jax.Array,\n",
    "  y: jax.Array,\n",
    "  *,\n",
    "  bm: int = 128,\n",
    "  bk: int = 128,\n",
    "  bn: int = 128,\n",
    "):\n",
    "  m, k = x.shape\n",
    "  _, n = y.shape\n",
    "\n",
    "  grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "    num_scalar_prefetch=0,\n",
    "    grid=(m//bm, n//bn, k//bk),\n",
    "    in_specs=[\n",
    "      pl.BlockSpec((bm, bk), lambda i,j,k: (i, k)),\n",
    "      pl.BlockSpec((bk, bn), lambda i,j,k: (k, j))\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),\n",
    "    scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)]\n",
    "  )\n",
    "\n",
    "  return pl.pallas_call(\n",
    "    functools.partial(simple_matmul, n_steps=k//bk),\n",
    "    grid_spec=grid_spec,\n",
    "    # Made this float32 to appease the numerics gods\n",
    "    out_shape=jax.ShapeDtypeStruct((m, n), dtype=jnp.bfloat16),\n",
    "    interpret=False\n",
    "  )(x, y)\n",
    "\n",
    "\n",
    "def distributed_gemm_kernel1(inputs, weights):\n",
    "  y_idx = jax.lax.axis_index('y')\n",
    "  # AG\n",
    "  w_full = jax.lax.all_gather(weights, 'x', axis=0, tiled=True)\n",
    "  # jax.debug.print('w_full: {}', w_full.shape)\n",
    "  # Slice out local arrays\n",
    "  # TODO: again, fix these so they're not tied to the specific shapes\n",
    "  w_slice = jax.lax.dynamic_slice(w_full, (y_idx * 1024, 0), (1024, 1024))\n",
    "  # jax.debug.print('w_slice: {}', w_slice.shape)\n",
    "  # jax.debug.print('input_shape: {}', inputs.shape)\n",
    "  # We'll take the default tile sizes for now\n",
    "  local_out = make_matmul(inputs, w_slice, bm=1024, bk=1024, bn=1024)\n",
    "  # jax.debug.print('local_out: {}', local_out.shape)\n",
    "  return jax.lax.psum(local_out, 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96f9e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgk1 = jax.jit(\n",
    "    jax.shard_map(\n",
    "    distributed_gemm_kernel1,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f26d7cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkResult (50 iters, 3 warmup)\n",
       "  mean:        0.295 ms\n",
       "  median:      0.294 ms\n",
       "  stdev:       0.007 ms\n",
       "  min:         0.278 ms\n",
       "  max:         0.317 ms\n",
       "  p95:         0.307 ms\n",
       "  p99:         0.313 ms"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jnp.allclose(dgk1(inputs, weights), jax_matmul(inputs, weights), rtol=1e-2, atol=1e-2)\n",
    "benchmark(dgk1, inputs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cc718331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumericsResult(PASS)\n",
       "  shape:     (2048, 1024)\n",
       "  max_diff:  0.000000\n",
       "  mean_diff: 0.000000\n",
       "  median:    0.000000\n",
       "  % > 0.1: 0.00%\n",
       "  worst at (0, 0): ref=-71.0000, test=-71.0000\n",
       "  regions (2x2):\n",
       "    [0,0]: mean=0.0000, %bad=0.0%\n",
       "    [0,1]: mean=0.0000, %bad=0.0%\n",
       "    [1,0]: mean=0.0000, %bad=0.0%\n",
       "    [1,1]: mean=0.0000, %bad=0.0%"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = jax_matmul(inputs, weights)\n",
    "test = dgk1(inputs, weights)\n",
    "\n",
    "numerics.compare(ref, test, atol=1e-2, rtol=1e-2, region_grid=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "804441f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_gather_kernel_1D(\n",
    "  input_ref, output_ref,\n",
    "  local_send_sem, send_sem, recv_sem, \n",
    "):\n",
    "  \"\"\"\n",
    "  input_ref: shard local data\n",
    "  output_ref: out shard\n",
    "\n",
    "  local_send_sem: allocates a semaphore for the local HBM copy\n",
    "  send_sem: semaphore for the RDMA push\n",
    "  recv_sem: semaphore for our local data\n",
    "  \"\"\"\n",
    "  # TODO: Barrier\n",
    "\n",
    "  pid = pl.program_id(0)\n",
    "  \n",
    "\n",
    "  shard_height = input_ref.shape[0]\n",
    "  shard_width = input_ref.shape[1]\n",
    "\n",
    "  # Get neighbors\n",
    "  x_ring = jax.lax.axis_size('x')\n",
    "  this_device_x = jax.lax.axis_index('x')\n",
    "  this_device_y = jax.lax.axis_index('y')\n",
    "  right_device_x = jax.lax.rem(this_device_x + 1, x_ring)\n",
    "\n",
    "  # Recall: This is the _destination_ copy slot\n",
    "  # TODO: Check and make sure these aren't garbage -> Think this might be correct on accident\n",
    "  copy_slot_xright = this_device_x - pid\n",
    "  copy_slot_xright = jax.lax.rem(copy_slot_xright + x_ring, x_ring)\n",
    "\n",
    "  # We're just copying within our HBM to a bigger HBM memory\n",
    "  @pl.when(pl.program_id(0) == 0)\n",
    "  def _copy_local_to_local():\n",
    "    local_hbm_copy = pltpu.make_async_copy(\n",
    "      src_ref=input_ref,\n",
    "      dst_ref=output_ref.at[pl.ds(this_device_x * shard_height, shard_height), :],\n",
    "      sem=local_send_sem\n",
    "    )\n",
    "\n",
    "    local_hbm_copy.start()\n",
    "    local_hbm_copy.wait()\n",
    "\n",
    "  right_dma = pltpu.make_async_remote_copy(\n",
    "    src_ref=output_ref.at[pl.ds(copy_slot_xright * shard_height, shard_height), :],\n",
    "    dst_ref=output_ref.at[pl.ds(copy_slot_xright * shard_height, shard_height), :],\n",
    "    send_sem=send_sem,\n",
    "    recv_sem=recv_sem,\n",
    "    device_id=(right_device_x, this_device_y),\n",
    "    device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "\n",
    "  right_dma.start()\n",
    "  right_dma.wait()\n",
    "  \n",
    "\n",
    "grid_spec_ag1d = pltpu.PrefetchScalarGridSpec(\n",
    "  num_scalar_prefetch=0,\n",
    "  grid=(1,),\n",
    "  in_specs=[\n",
    "    # Our input reference is just our big tensor in HBM\n",
    "    pl.BlockSpec(memory_space=pl.ANY)\n",
    "  ],\n",
    "  # Our output reference will be _another_ big tensor in HBM\n",
    "  out_specs=pl.BlockSpec(memory_space=pl.ANY),\n",
    "  scratch_shapes=(\n",
    "    [pltpu.SemaphoreType.DMA] * 2 # local_copy_op, send_sem\n",
    "    + [pltpu.SemaphoreType.DMA] * 1 # These are our recv_sems. For 2x2, we only need 1 of them\n",
    "  )\n",
    ")\n",
    "\n",
    "out_shape_ag1d=jax.ShapeDtypeStruct((weights.shape), dtype=jnp.bfloat16)\n",
    "\n",
    "def make_ag(x, interpret: None | bool = None):\n",
    "  if not interpret:\n",
    "    platform = jax.devices()[0].platform\n",
    "    if platform == 'tpu':\n",
    "      interpret = False\n",
    "    else:\n",
    "      # ignoring gpu for now\n",
    "      interpret=True\n",
    "  \n",
    "  return pl.pallas_call(\n",
    "    all_gather_kernel_1D,\n",
    "    grid_spec=grid_spec_ag1d,\n",
    "    out_shape=out_shape_ag1d,\n",
    "    interpret=interpret\n",
    "  )(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ff4b36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xla_ag = jax.jit(\n",
    "    jax.shard_map(\n",
    "        lambda x: jax.lax.all_gather(x, 'x', tiled=True),\n",
    "        mesh=mesh, in_specs=P('x', None), out_specs=P('x', None))\n",
    ")(weights)\n",
    "\n",
    "ag1d = jax.jit(\n",
    "  jax.shard_map(\n",
    "      make_ag,\n",
    "      mesh=mesh,\n",
    "      in_specs=P('x', None),\n",
    "      out_specs=P('x', None),\n",
    "      check_vma=False\n",
    "  )\n",
    ")(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5002183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: When I was trying to directly use \n",
    "`output_ref[...] = local_hbm_ref[...] + output_ref[...]` it was causing an error.\n",
    "> ValueError: Loads are only allowed on VMEM and SMEM references. ANY memory space can only be accessed using async_copy.\n",
    "\n",
    "Reason:\n",
    "output_ref is marked at pl.ANY. Only VMEM/SMEM are addressable by compute units (ref[...] compiles to loads/stores)\n",
    "Off-chip memory (HBM; pl.ANY) accessible via DMAs, not loads/stores\n",
    "\"\"\"\n",
    "\n",
    "def all_reduce_kernel_1D(\n",
    "    local_hbm_ref, output_ref,\n",
    "    send_sem, recv_sem,\n",
    "    local_scratch, recv_scratch, copy_sem\n",
    "):\n",
    "    \"\"\"\n",
    "    Right now, here's what we'll have --\n",
    "    We have all-gathered the full weight tensor into each device's HBM\n",
    "    THEN: we will have compute a GEMM over the device-local chunks\n",
    "    We need to all-reduce over the Y AXIS at the end so that\n",
    "    all the data is in the right place/on the right device\n",
    "\n",
    "    The data needs to go from P('x', NONE) -> P('x', None)\n",
    "    Basically -> We did GEMMs on inputs[0:M/2, 0:N/2] @ weights[0:M/2,N], ...\n",
    "\n",
    "    We need to sum those row stipes over the y-axis, and everything will\n",
    "    be good to go\n",
    "\n",
    "    This will be _SLOW_ for now because of the HBM traffic\n",
    "    \"\"\"\n",
    "\n",
    "    y_ring = jax.lax.axis_size('y')\n",
    "    this_device_x = jax.lax.axis_index('x')\n",
    "    this_device_y = jax.lax.axis_index('y')\n",
    "    right_device_y = jax.lax.rem(this_device_y + 1, y_ring)\n",
    "\n",
    "    local_copy = pltpu.make_async_copy(\n",
    "       src_ref=local_hbm_ref,\n",
    "       dst_ref=local_scratch,\n",
    "       sem=copy_sem\n",
    "    )\n",
    "    local_copy.start()\n",
    "\n",
    "    # This will copy our HBM tile into either:\n",
    "    #  - Remote HBM Tile\n",
    "    #    - Right now our GEMM works on HBM, so this will be easier temorarily\n",
    "    #  - Remote VMEM tile (Memory pressure)\n",
    "    right_dma = pltpu.make_async_remote_copy(\n",
    "        src_ref=local_hbm_ref,\n",
    "        dst_ref=output_ref,\n",
    "        send_sem=send_sem,\n",
    "        recv_sem=recv_sem,\n",
    "        device_id=(this_device_x, right_device_y),\n",
    "        device_id_type=pltpu.DeviceIdType.MESH\n",
    "    )\n",
    "\n",
    "    right_dma.start()\n",
    "    local_copy.wait()\n",
    "    right_dma.wait()\n",
    "\n",
    "    # output_ref[...] = local_hbm_ref[...] + output_ref[...]\n",
    "    # Add in VMEM, write back to HBM\n",
    "    local_scratch[...] = local_scratch[...] + recv_scratch[...]\n",
    "\n",
    "    out_copy = pltpu.make_async_copy(\n",
    "          src_ref=local_scratch,\n",
    "          dst_ref=output_ref,\n",
    "          sem=copy_sem\n",
    "      )\n",
    "    out_copy.start()\n",
    "    out_copy.wait()\n",
    "\n",
    "\n",
    "grid_spec_ar1d=pltpu.PrefetchScalarGridSpec(\n",
    "    num_scalar_prefetch=0,\n",
    "    grid=(1,),\n",
    "    in_specs=[\n",
    "        pl.BlockSpec(memory_space=pl.ANY),\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec(memory_space=pl.ANY),\n",
    "    scratch_shapes=(\n",
    "        [pltpu.SemaphoreType.DMA] * 3 # send, recv, copy\n",
    "        + [pltpu.VMEM((1024, 1024), jnp.bfloat16)] # local scratch\n",
    "        + [pltpu.VMEM((1024, 1024), jnp.bfloat16)] # recv scratch\n",
    "    )\n",
    ")\n",
    "\n",
    "out_shape_ar1d = jax.ShapeDtypeStruct((weights.shape), dtype=jnp.bfloat16)\n",
    "\n",
    "\"\"\"\n",
    "Notice how you can also achieve the automatic HBM pipelining with blockspecs\n",
    "as you might with a GEMM\n",
    "\n",
    "def all_reduce_kernel_1D(\n",
    "    local_ref,      # VMEM (auto-copied from HBM by Pallas)\n",
    "    output_ref,     # VMEM (auto-copied to HBM by Pallas)\n",
    "    recv_scratch,   # VMEM scratch\n",
    "    send_sem, recv_sem\n",
    "):\n",
    "    y_ring = jax.lax.axis_size('y')\n",
    "    this_device_x = jax.lax.axis_index('x')\n",
    "    this_device_y = jax.lax.axis_index('y')\n",
    "    right_device_y = jax.lax.rem(this_device_y + 1, y_ring)\n",
    "\n",
    "    right_dma = pltpu.make_async_remote_copy(\n",
    "        src_ref=local_ref,\n",
    "        dst_ref=recv_scratch,\n",
    "        send_sem=send_sem,\n",
    "        recv_sem=recv_sem,\n",
    "        device_id=(this_device_x, right_device_y),\n",
    "        device_id_type=pltpu.DeviceIdType.MESH\n",
    "    )\n",
    "    right_dma.start()\n",
    "    right_dma.wait()\n",
    "\n",
    "    output_ref[...] = local_ref[...] + recv_scratch[...]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def make_ar(input_array, interpret: None | bool = None):\n",
    "\n",
    "    # ar_grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "    # num_scalar_prefetch=0,\n",
    "    # grid=(1,),\n",
    "    # in_specs=[pl.BlockSpec((1024, 1024), lambda i: (0, 0))],\n",
    "    # out_specs=pl.BlockSpec((1024, 1024), lambda i: (0, 0)),\n",
    "    # scratch_shapes=(\n",
    "    #     [pltpu.VMEM((1024, 1024), jnp.bfloat16)]  # recv_scratch\n",
    "    #     + [pltpu.SemaphoreType.DMA] * 2\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    out_shape = jax.ShapeDtypeStruct(input_array.shape, input_array.dtype)\n",
    "\n",
    "    return pl.pallas_call(\n",
    "        all_reduce_kernel_1D,\n",
    "        grid_spec=grid_spec_ar1d,\n",
    "        out_shape=out_shape,\n",
    "        # interpret=interpret\n",
    "    )(input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f5ed90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_ag_gemm_ar_kernel(inputs, weights):\n",
    "    y_idx = jax.lax.axis_index('y')\n",
    "    a = make_ag(weights)\n",
    "    a_slice = jax.lax.dynamic_slice(a, (y_idx * 1024, 0), (1024,1024))\n",
    "    b = make_matmul(inputs, a_slice, bm=1024, bk=1024, bn=1024)\n",
    "    c = make_ar(b)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3942ea10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-71, -42.5, -7.875, ..., 22.5, 85, 40],\n",
       "       [8.875, -40, 21, ..., -27.75, 74.5, -51.5],\n",
       "       [-36.75, -18.625, 90, ..., -70, -34, 5.625],\n",
       "       ...,\n",
       "       [-1.75, -9.625, 20.75, ..., 47.25, -47.5, 1.90625],\n",
       "       [-7.21875, 30.375, -26, ..., 11, -95, -66.5],\n",
       "       [-11, -24.375, 48, ..., -72.5, -17.625, 18.875]], dtype=bfloat16)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagak = jax.jit(\n",
    "    jax.shard_map(\n",
    "        slow_ag_gemm_ar_kernel,\n",
    "        mesh=mesh,\n",
    "        in_specs=(P('x', 'y'), P('x', None)),\n",
    "        out_specs=P('x', None),\n",
    "        check_vma=False\n",
    "    )\n",
    ")\n",
    "\n",
    "sagak(inputs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.6 ms\n",
      "BenchmarkResult (50 iters, 3 warmup)\n",
      "  mean:        0.289 ms\n",
      "  median:      0.289 ms\n",
      "  stdev:       0.008 ms\n",
      "  min:         0.274 ms\n",
      "  max:         0.308 ms\n",
      "  p95:         0.302 ms\n",
      "  p99:         0.306 ms\n"
     ]
    }
   ],
   "source": [
    "# NOTE: These benchmarks look dominated by overhead\n",
    "# Need to rely on profile for accurate information\n",
    "result = benchmark(sagak, inputs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2a9029cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkResult (50 iters, 3 warmup)\n",
       "  mean:        0.287 ms\n",
       "  median:      0.286 ms\n",
       "  stdev:       0.009 ms\n",
       "  min:         0.273 ms\n",
       "  max:         0.325 ms\n",
       "  p95:         0.300 ms\n",
       "  p99:         0.320 ms"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(jax_matmul, inputs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343876ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagak_compiled = sagak.lower(inputs, weights).compile({'xla_enable_transpose_trace': True})\n",
    "result = sagak_compiled(inputs, weights)\n",
    "result.block_until_ready()\n",
    "\n",
    "with jax.profiler.trace('./traces'):\n",
    "    result = sagak_compiled(inputs, weights)\n",
    "    result.block_until_ready()\n",
    "\n",
    "\"\"\"\n",
    "113 us vs 208 us, slow but workable\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3954349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basically --\n",
    "The way we organize the grid for communications is a decision\n",
    "  - Rings, ranges, etc.\n",
    "  - This trades off bandwidth/latency\n",
    "\n",
    "\n",
    "# Ring\n",
    "grid = (ring_size - 1,)\n",
    "\n",
    "# Recursive doubling\n",
    "grid = (log2(ring_size),)\n",
    "\n",
    "# Direct\n",
    "grid = (1,)  # but more complex RDMA pattern\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5480de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ValueError: The context mesh AbstractMesh('x': 2, 'y': 2, axis_types=(Manual, Manual),\n",
    "  device_kind=TPU v5 lite, num_cores=1) should match the mesh passed to shard_map Mesh('x': 2,\n",
    "   'y': 2, axis_types=(Auto, Auto))\n",
    "\"\"\"\n",
    "\n",
    "@functools.partial(\n",
    "    jax.shard_map,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False\n",
    ")\n",
    "def xla_matmul_4(input: jax.Array, weight: jax.Array) -> jax.Array:\n",
    "    y_idx = jax.lax.axis_index('y')\n",
    "    w_full = mag1(weight)\n",
    "    # Using the y-ring axis to determined which col stripe of weights to compute locally\n",
    "    w_slice = jax.lax.dynamic_slice(w_full, (y_idx * 1024, 0), (1024, 1024))\n",
    "    local_out = make_matmul(inputs, w_slice)\n",
    "    # All Reduce over the y-ring to accumulate partial results\n",
    "    out = jax.lax.psum(local_out, 'y')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc015c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here's something fun --\n",
    "\n",
    "NotImplementedError: Meshes with more than 1 named\n",
    "dimension not implemented in dma_start_p\n",
    "\n",
    "Pallas's remote DMA primitives currently only support 1D meshes\n",
    "\n",
    "https://github.com/jax-ml/jax/blob/main/jax/_src/pallas/mosaic/primitives.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTEND THE KERNEL TO BE MORE GENERAL THAN 2x2 ; SCALE UP TO 4x4 GRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebaf2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "On each local device, we have 2 arrays sitting there -> inputs + weights\n",
    "On kernel start:\n",
    "  - Barrier sync to get everyone on the same stage\n",
    "    - Might could relax this constraint?\n",
    "  - Issue remote DMAs along the ... ring to all gather the weights\n",
    "  - Once received\n",
    "    - Compute local dots\n",
    "    - Accumulate\n",
    "  - Finally\n",
    "    - All Reduce over the y ring to conform shmap shape\n",
    "\n",
    "\n",
    "Additional notes:\n",
    "  RECALL: n_hopes = grid (ring_len) - 1 -> I think this is just using one ICI link though\n",
    "  What happens at n = 1? 2? 3?\n",
    "  n=1:\n",
    "    - You send right, receive left (this kernel is only looking at 1D case rn)\n",
    "    - At (0,0): You send to (1,0) the chunk starting at 0 * row_height\n",
    "    - You receive chunks from (1,0) and (3, 0)\n",
    "    - You have [x, x, O, x] ; need idx 2 * row_height\n",
    "    - r_neighbor needs -> [x, x, x, O]\n",
    "  n=2:\n",
    "    - This would be the epilogue already w/ 2 ICI links**\n",
    "    - You send r_neighbor+1 chunk ; could probably push this up to initial mappings\n",
    "\"\"\"\n",
    "\n",
    "def all_gather_kernel_bidi(\n",
    "  input_ref, output_ref,\n",
    "  local_send_sem, send_sem_right, send_sem_left,\n",
    "  recv_sem_right, recv_sem_left\n",
    "):\n",
    "  \"\"\"\n",
    "  input_ref: shard local data\n",
    "  output_ref: out shard\n",
    "\n",
    "  local_send_sem: allocates a semaphore for the local HBM copy\n",
    "  send_sem: semaphore for the RDMA push\n",
    "  recv_sem: semaphore for our local data\n",
    "  \"\"\"\n",
    "  # TODO: Barrier\n",
    "\n",
    "  pid = pl.program_id(0)\n",
    "  \n",
    "  # This should be baked into compiled artifact? Or is it runtime?\n",
    "  # There has to be a prettier way to do this?\n",
    "  shard_height = input_ref.shape[0]\n",
    "  shard_width = input_ref.shape[1]\n",
    "\n",
    "  this_device_x = jax.lax.axis_index('x')\n",
    "  this_device_y = jax.lax.axis_index('y')\n",
    "  x_ring = jax.lax.axis_size('x')\n",
    "  right_device_x = jax.lax.rem(this_device_x + 1, x_ring)\n",
    "  left_device_x = jax.lax.rem(this_device_x - 1 + x_ring, x_ring)\n",
    "\n",
    "  # y_ring = jax.lax.axis_size('y')\n",
    "  # right_device_y = jax.lax.rem(this_device_y + 1, y_ring)\n",
    "  # left_device_y = jax.lax.rem(this_device_y - 1 + y_ring, y_ring)\n",
    "\n",
    "\n",
    "  # This accounts for the offset when data is being sent both ways\n",
    "  copy_slot_right = this_device_x - pid\n",
    "  copy_slot_right = jax.lax.rem(copy_slot_right + x_ring, x_ring)\n",
    "  copy_slot_left = jax.lax.rem(this_device_x + pid, x_ring)\n",
    "\n",
    "\n",
    "  local_copy = local_hbm_copy = pltpu.make_async_copy(\n",
    "    src_ref=input_ref,\n",
    "    dst_ref=output_ref.at[pl.ds(this_device_x * shard_height, shard_height), :],\n",
    "    sem=local_send_sem\n",
    ")\n",
    "\n",
    "  # PERFORM INITIAL ASYNC COPY FROM OUR HBM TO OUR HBM\n",
    "  # We're just copying within our HBM to a bigger HBM memory\n",
    "  # XLA liveness should handle malloc/free the _INPUT_ tensor once the AG completes\n",
    "  @pl.when(pl.program_id(0) == 0)\n",
    "  def _copy_local_to_local():\n",
    "    # We can defer the wait until literally the very end of the kernel\n",
    "    # TODO: We can defer this copy and make it free**\n",
    "    # Need to add 2 more DMAs that send from input_ref -> remote output_ref at\n",
    "    # the beginning, rather than output_ref -> output_ref\n",
    "    # This is needlessly serial right now\n",
    "    local_copy.start()\n",
    "    local_copy.wait()\n",
    "\n",
    "  right_dma = pltpu.make_async_remote_copy(\n",
    "    # Next kernel iter depends on completion of left/right DMAs\n",
    "    src_ref=output_ref.at[pl.ds(copy_slot_right * shard_height, shard_height), :],\n",
    "    dst_ref=output_ref.at[pl.ds(copy_slot_right * shard_height, shard_height), :],\n",
    "    send_sem=send_sem_right,\n",
    "    # Imagine this as an array of semaphores-> [Dev1, Dev2, Dev3, Dev4, ..., DevN]\n",
    "    # Signals the semaphore on the _destination_ device to signal\n",
    "    recv_sem=recv_sem_right,\n",
    "    device_id=(right_device_x, this_device_y),\n",
    "    device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "\n",
    "  left_dma = pltpu.make_async_remote_copy(\n",
    "    src_ref=output_ref.at[pl.ds(copy_slot_left * shard_height, shard_height), :],\n",
    "    dst_ref=output_ref.at[pl.ds(copy_slot_left * shard_height, shard_height), :],\n",
    "    send_sem=send_sem_left,\n",
    "    recv_sem=recv_sem_left,\n",
    "    device_id=(left_device_x, this_device_y),\n",
    "    device_id_type=pltpu.DeviceIdType.MESH\n",
    "  )\n",
    "\n",
    "  right_dma.start()\n",
    "  left_dma.start()\n",
    "  right_dma.wait()\n",
    "  left_dma.wait()\n",
    "  \n",
    "\n",
    "grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "  num_scalar_prefetch=0,\n",
    "  # If you're using both ICI links:\n",
    "  # grid=ceil((num_devices-1) / 2,) -> You're halving the num iters\n",
    "  # grid=((ring_size-1) / 2)\n",
    "  grid=(1,),\n",
    "  in_specs=[\n",
    "    # Our input reference is just our big tensor in HBM\n",
    "    pl.BlockSpec(memory_space=pl.ANY)\n",
    "  ],\n",
    "  # Our output reference will be _another_ big tensor in HBM\n",
    "  out_specs=pl.BlockSpec(memory_space=pl.ANY),\n",
    "  # This will be an error if you need more semaphores for more neighbors\n",
    "  scratch_shapes=(\n",
    "    [pltpu.SemaphoreType.DMA] * 3 # local_copy_op, send_sem_left, send_sem_right\n",
    "    + [pltpu.SemaphoreType.DMA] * 2 # These are our recv_sems (For 2x2, we only need 1 of them)\n",
    "  )\n",
    ")\n",
    "\n",
    "out_shape=jax.ShapeDtypeStruct((weights.shape), dtype=jnp.bfloat16)\n",
    "\n",
    "\n",
    "# TODO: should we parameterize this _here_; aka pass in the shard shape data here?\n",
    "def make_ag(x, interpret: None | bool = None):\n",
    "  if not interpret:\n",
    "    platform = jax.devices()[0].platform\n",
    "    if platform == 'tpu':\n",
    "      interpret = False\n",
    "    else:\n",
    "      # ignoring gpu for now\n",
    "      interpret=True\n",
    "  \n",
    "  return pl.pallas_call(\n",
    "    all_gather_kernel_bidi,\n",
    "    grid_spec=grid_spec,\n",
    "    out_shape=out_shape,\n",
    "    interpret=interpret\n",
    "  )(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf16a2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xla_ag = jax.jit(\n",
    "    jax.shard_map(\n",
    "        lambda x: jax.lax.all_gather(x, 'x', tiled=True),\n",
    "        mesh=mesh, in_specs=P('x', None), out_specs=P('x', None))\n",
    ")(weights)\n",
    "\n",
    "agbd = jax.jit(\n",
    "  jax.shard_map(\n",
    "      make_ag,\n",
    "      mesh=mesh,\n",
    "      in_specs=P('x', None),\n",
    "      out_specs=P('x', None),\n",
    "      check_vma=False\n",
    "  )\n",
    ")(weights)\n",
    "\n",
    "jnp.allclose(xla_ag, agbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is where we introduce semaphores as a way to mark data dependenices\n",
    "# I.e., Results from one computation _depend_ on previous results**\n",
    "# the semaphore lives on the device where wait() will be called**\n",
    "\n",
    "def all_reduce_bidi(\n",
    "    input_ref, output_ref,\n",
    "    local_send_sem, send_sem_right, send_sem_left,\n",
    "    recv_sem_right, recv_sem_left,\n",
    "    hbm_scratch # we're sending into hbm scratch (for now)\n",
    "):\n",
    "  shard_height = input_ref.shape[0]\n",
    "  shard_width = input_ref.shape[1]\n",
    "\n",
    "  pid = pl.program_id(0)\n",
    "\n",
    "  x_ring = jax.lax.axis_size('x')\n",
    "  y_ring = jax.lax.axis_size('y')\n",
    "  this_device_y = jax.lax.axis_index('y')\n",
    "  this_device_x = jax.lax.axis_index('x')\n",
    "\n",
    "  left_device_x = (this_device_x - 1) % x_ring\n",
    "  right_device_y = (this_device_y + 1) % y_ring\n",
    "\n",
    "  copy_slot_right = this_device_y - pid\n",
    "  copy_slot_right = jax.lax.rem(copy_slot_right + y_ring, y_ring)\n",
    "  copy_slot_left = jax.lax.rem(this_device_y + pid, y_ring)\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  - Send directly to remote VMEM\n",
    "    - Overlap comms with local read latency\n",
    "  - This is where you need to be careful about synchronizing**\n",
    "    - This probably requires capacity semaphores\n",
    "    - UNLESS you have enough space in VMEM to hold all of the data...\n",
    "      - But even then you're just waiting to accumulate...\n",
    "  - You can still pipeline this pretty aggressively with bidi\n",
    "  - But this is also the copy/receiving concept**\n",
    "    - Work from one slot, receive on another\n",
    "  \"\"\"\n",
    "\n",
    "  right_dma = pltpu.make_async_remote_copy(\n",
    "    src_ref=input_ref.at[:, pl.ds(copy_slot_right * shard_width)],\n",
    "    dst_ref=hbm_scratch.at[:, pl.ds(copy_slot_right * shard_width)],\n",
    "    send_sem=send_sem_right,\n",
    "    recv_sem=,\n",
    "    device_id=(this_device_x, this_device_y + 1),\n",
    "    device_id_type=pltpu.DeviceIdType.MESH\n",
    "  )\n",
    "\n",
    "  left_dma = pltpu.make_async_remote_copy(\n",
    "    src_ref=,\n",
    "    dst_ref=,\n",
    "    send_sem=,\n",
    "    recv_sem=,\n",
    "    device_id=(this_device_x, this_device_y - 1),\n",
    "    device_id_type=pltpu.DeviceIdType.MESH\n",
    "  )\n",
    "\n",
    "grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "    num_scalar_prefetch=0,\n",
    "    # TODO: this is wrong beyond 2x2\n",
    "    grid=(1,),\n",
    "    in_specs=[\n",
    "        pl.BlockSpec()\n",
    "    ],\n",
    "    out_specs=(\n",
    "        jax.ShapeDtypeStruct((weights.shape), dtype=jnp.bfloat16),\n",
    "    ),\n",
    "    scratch_shapes=(\n",
    "      # pl.ANY()\n",
    "    )\n",
    ")\n",
    "\n",
    "def make_ar_bidi(x):\n",
    "    return pl.pallas_call(\n",
    "        all_reduce_bidi,\n",
    "        grid_spec=grid_spec,\n",
    "        out_shape=out_shape,\n",
    "        # compiler_params=pltpu.CompilerParams(\n",
    "            # collective_id=0\n",
    "        # )\n",
    "    )(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added granularity of halving the outgoing ICI copies (L/R)\n",
    "# NOTE: SHRINKING THE STEP SIZES (data sent) CAN BETTER OVERLAP??\n",
    "# Except ICI is slowest comms channel ; this is good to test experimentally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisiting the matmul\n",
    "# Might not need this except to explain tweaks from within kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c560155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instead of AG  slice  GEMM sequentially, overlap the weight\n",
    "transfer with GEMM computation.\n",
    "\n",
    "Each device (i, j) has:\n",
    "  inputs[I_x, J_y]  (1024, 1024)\n",
    "  weights[J_x, K]   (1024, 1024)\n",
    "\n",
    "The correct partial product is: inputs[i,j] @ weights[j,:]\n",
    "After AR over y: output[i,:] = sum_j inputs[i,j] @ weights[j,:]\n",
    "\n",
    "For the AG over x (x_ring=2):\n",
    "  x_idx == y_idx; local weights ARE weights[j,:], compute immediately\n",
    "  x_idx != y_idx; need neighbor's weights, wait for RDMA\n",
    "\n",
    "Start RDMA, run local GEMM (overlapping comm+compute on the devices that\n",
    "already have the right chunk), then recompute with received weights only where needed.\n",
    "\"\"\"\n",
    "\n",
    "def _emit_gemm(x_ref, w_ref, o_ref, *, bm, bk, bn):\n",
    "    \"\"\"\n",
    "    Emit a tiled GEMM pipeline\n",
    "    All refs are HBM. emit_pipeline handles VMEM tiling + double-buffering.\n",
    "    \"\"\"\n",
    "    m, k_dim = x_ref.shape\n",
    "    _, n = w_ref.shape\n",
    "    grid = (m // bm, n // bn, k_dim // bk)\n",
    "\n",
    "    def body(x_vmem, w_vmem, o_vmem, accum):\n",
    "        @pl.when(pl.program_id(2) == 0)\n",
    "        def _():\n",
    "            accum[...] = jnp.zeros_like(accum)\n",
    "\n",
    "        accum[...] += jnp.dot(\n",
    "            x_vmem[...], w_vmem[...],\n",
    "            preferred_element_type=jnp.float32,\n",
    "        )\n",
    "\n",
    "        @pl.when(pl.program_id(2) == pl.num_programs(2) - 1)\n",
    "        def _():\n",
    "            o_vmem[...] = accum[...].astype(o_vmem.dtype)\n",
    "\n",
    "    @functools.partial(pl.run_scoped, accum=pltpu.VMEM((bm, bn), jnp.float32))\n",
    "    def _(accum):\n",
    "        pltpu.emit_pipeline(\n",
    "            functools.partial(body, accum=accum),\n",
    "            grid=grid,\n",
    "            in_specs=[\n",
    "                pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)),\n",
    "                pl.BlockSpec((bk, bn), lambda i, j, k: (k, j)),\n",
    "            ],\n",
    "            out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),\n",
    "        )(x_ref, w_ref, o_ref)\n",
    "\n",
    "\n",
    "def fused_ag_gemm_kernel(\n",
    "    input_ref,          # HBM: inputs shard (m_local, k_local)\n",
    "    weight_ref,         # HBM: weights shard (k_local, n)\n",
    "    output_ref,         # HBM: GEMM output (m_local, n)\n",
    "    recv_weight_ref,    # HBM: workspace for received weights (k_local, n)\n",
    "    send_sem, recv_sem, # RDMA semaphores\n",
    "):\n",
    "    \"\"\"\n",
    "    Fused AG + GEMM via collective permute.\n",
    "\n",
    "    Outer kernel manages weight exchange (RDMA along x-ring).\n",
    "    Inner pipelines handle the tiled matmul.\n",
    "\n",
    "    On half the devices (x_idx == y_idx) the local weights are\n",
    "    already correct, so GEMM runs entirely overlapped with RDMA.\n",
    "    On the other half (x_idx != y_idx) we wait for the remote\n",
    "    chunk and then recompute  still a win over a blocking AG\n",
    "    because the first GEMM warmed the MXU pipeline.\n",
    "    \"\"\"\n",
    "    x_idx = jax.lax.axis_index('x')\n",
    "    y_idx = jax.lax.axis_index('y')\n",
    "    x_ring = jax.lax.axis_size('x')\n",
    "    right_neighbor = jax.lax.rem(x_idx + 1, x_ring)\n",
    "\n",
    "    BM, BK, BN = 128, 128, 128\n",
    "\n",
    "    # --- Step 1: Kick off async weight exchange along x-ring ---\n",
    "    # Each device sends its shard right and receives from its left neighbor\n",
    "    rdma = pltpu.make_async_remote_copy(\n",
    "        src_ref=weight_ref,\n",
    "        dst_ref=recv_weight_ref,\n",
    "        send_sem=send_sem,\n",
    "        recv_sem=recv_sem,\n",
    "        device_id=(right_neighbor, y_idx),\n",
    "        device_id_type=pltpu.DeviceIdType.MESH,\n",
    "    )\n",
    "    rdma.start()\n",
    "\n",
    "    # --- Step 2: Local GEMM (overlaps RDMA) ---\n",
    "    # When x_idx == y_idx this IS the correct result.\n",
    "    # When x_idx != y_idx this is wasted compute  but the MXU\n",
    "    # stays busy while ICI transfers the weight shard we need.\n",
    "    _emit_gemm(input_ref, weight_ref, output_ref, bm=BM, bk=BK, bn=BN)\n",
    "\n",
    "    # --- Step 3: Wait for remote weights ---\n",
    "    rdma.wait()\n",
    "\n",
    "    # --- Step 4: Recompute with correct weights where needed ---\n",
    "    # NOTE: if @pl.when around emit_pipeline gives trouble, the\n",
    "    # fallback is to always copy the correct chunk into\n",
    "    # recv_weight_ref (local or received) and run one GEMM.\n",
    "    @pl.when(x_idx != y_idx)\n",
    "    def _():\n",
    "        _emit_gemm(input_ref, recv_weight_ref, output_ref, bm=BM, bk=BK, bn=BN)\n",
    "\n",
    "\n",
    "def make_fused_ag_gemm(inputs, weights):\n",
    "    m_local, k_local = inputs.shape\n",
    "    _, n = weights.shape\n",
    "\n",
    "    grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=0,\n",
    "        grid=(1,),\n",
    "        in_specs=[\n",
    "            pl.BlockSpec(memory_space=pl.ANY),  # inputs\n",
    "            pl.BlockSpec(memory_space=pl.ANY),  # weights\n",
    "        ],\n",
    "        # Two outputs: real output + HBM workspace for received weights\n",
    "        out_specs=[\n",
    "            pl.BlockSpec(memory_space=pl.ANY),  # GEMM output\n",
    "            pl.BlockSpec(memory_space=pl.ANY),  # recv weight buffer\n",
    "        ],\n",
    "        scratch_shapes=[\n",
    "            [pltpu.SemaphoreType.DMA] * 2,  # send_sem, recv_sem\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    out_shape = [\n",
    "        jax.ShapeDtypeStruct((m_local, n), inputs.dtype),   # GEMM output\n",
    "        jax.ShapeDtypeStruct((k_local, n), weights.dtype),  # recv workspace\n",
    "    ]\n",
    "\n",
    "    results = pl.pallas_call(\n",
    "        fused_ag_gemm_kernel,\n",
    "        grid_spec=grid_spec,\n",
    "        out_shape=out_shape,\n",
    "    )(inputs, weights)\n",
    "\n",
    "    return results[0]  # discard the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3365c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUT IT TOGETHER\n",
    "# INTERLEAVE THE COMPUTE with ppermute\n",
    "# This is where the fun begins... How many versions can we cook up?\n",
    "# We want blocks that are multiples of (8, 128) -> Benefit from larger block sizes\n",
    "# Largely because we have FOUR MXUs on TPUv5e -> Only 2 for TPUv6e, but they're 256x256\n",
    "\n",
    "def fused_ag_gemm_ar(inputs, weights):\n",
    "    partial = make_fused_ag_gemm(inputs, weights)\n",
    "    return make_ar(partial)\n",
    "\n",
    "fused_fn = jax.jit(jax.shard_map(\n",
    "    fused_ag_gemm_ar,\n",
    "    mesh=mesh,\n",
    "    in_specs=(P('x', 'y'), P('x', None)),\n",
    "    out_specs=P('x', None),\n",
    "    check_vma=False,\n",
    "))\n",
    "\n",
    "fused_fn(inputs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb556444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMERICS CHECK\n",
    "ref = jax_matmul(inputs, weights)\n",
    "test = fused_fn(inputs, weights)\n",
    "numerics.compare(ref, test, atol=1e-2, rtol=1e-2, region_grid=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE CHECK\n",
    "benchmark(fused_fn, inputs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c628a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROFILING\n",
    "# TODO: Don't forget to add annotations + compile with --xla-enable-transpose-trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77392018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: INCLUDE A VERSION WITH SOME XLA COMPILER FLAGS ENABLED TO TEST PERFORMANCE OR ASYNC ALL_GATHER, ETC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
